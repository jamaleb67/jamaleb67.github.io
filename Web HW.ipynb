{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely,  let's break down a web scraping in-class exercise tailored for an Intelligence Community (IC) perspective, focused on the Russia-Ukraine conflict.\n",
    "\n",
    "**Title:** Divergent Perspectives: Web Scraping the Russia-Ukraine Conflict\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "*   **Understanding Media Bias:** Learn how different news outlets frame the same events through selective reporting and language.\n",
    "*   **Intelligence Gathering:** Extract key information on troop movements, casualties, equipment, and political statements to enhance situational awareness.\n",
    "*   **Critical Analysis:**  Develop skills to sift through narratives and identify information potentially used for propaganda or disinformation.\n",
    "\n",
    "**Technical Setup**\n",
    "\n",
    "*   **Programming Language:**  Python (its popularity and ease of use are ideal for in-class)\n",
    "*   **Libraries:**\n",
    "    *   **Requests:** For fetching website HTML content.\n",
    "    *   **Beautiful Soup 4:**  For parsing and extracting data from HTML.\n",
    "    *   **Pandas:** For storing and manipulating scraped data (optional).\n",
    "\n",
    "**Project Outline**\n",
    "\n",
    "1.  **Website Selection**\n",
    "    *   Students research and select three news websites:\n",
    "        *   **Pro-Russian slant:**  Ex: TASS (Russian state-owned), RT, SouthFront\n",
    "        *   **Pro-Ukrainian slant:**  Ex: The Kyiv Independent, Ukrinform\n",
    "        *   **International focus:** Ex: BBC World News, Al Jazeera, Reuters\n",
    "    \n",
    "2.  **Target Elements Identification:**\n",
    "    *   **Headlines:**  Capture overall messaging\n",
    "    *   **Articles:** For in-depth analysis, identifying key events, locations, and figures.\n",
    "    *   **Dates:** To timeline events and spot evolving narratives.\n",
    "    *   **Author:** To attribute viewpoints and consider potential affiliations.\n",
    "\n",
    "3.  **Web Scraping Script Development:**\n",
    "    *   **Inspect webpage structure:** Students use browser developer tools to pinpoint the HTML tags containing the target elements.\n",
    "    *   **Write targeted Python scripts:** Employ Requests and Beautiful Soup to:\n",
    "        *   Fetch HTML from each site.\n",
    "        *   Parse the HTML, isolating target elements based on tags/classes.\n",
    "        *   Extract the text content.\n",
    "\n",
    "4.  **Data Storage and Analysis**\n",
    "    *   **Storage:** Save the results:\n",
    "        *   Simple: CSV or text files.\n",
    "        *   Advanced (optional): Use Pandas dataframes for easier manipulation.\n",
    "    *   **Analysis:** Guide students to perform:\n",
    "        *   **Comparative Word Analysis:** Word clouds, keyword frequencies to pinpoint differing terminology\n",
    "        *   **Sentiment Analysis:**  Tools to estimate positive/negative tone in the coverage. \n",
    "        *   **Timeline Visualizations:** Plotting events according to their reporting dates.\n",
    "\n",
    "**In-Class Discussion:** \n",
    "\n",
    "*   **How does each outlet frame the same event?** Look for specific word choices, omitted details, and the prominence given to particular narratives.\n",
    "*   **Identifying potential disinformation:** Does any outlet spread demonstrably false information or promote unsubstantiated claims?\n",
    "*   **IC Applications:**  How can similar techniques be used in real-world IC monitoring of evolving international situations?\n",
    "\n",
    "**Ethical Considerations**\n",
    "\n",
    "Emphasize the importance of responsible web scraping, respecting website terms of service, and avoiding excessive requests that could overload servers.\n",
    "\n",
    "**Let me know if you'd like assistance crafting the actual Python code examples or want to explore more advanced functionalities within this exercise!** \n",
    "Absolutely,  let's break down a web scraping in-class exercise tailored for an Intelligence Community (IC) perspective, focused on the Russia-Ukraine conflict.\n",
    "\n",
    "**Title:** Divergent Perspectives: Web Scraping the Russia-Ukraine Conflict\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "*   **Understanding Media Bias:** Learn how different news outlets frame the same events through selective reporting and language.\n",
    "*   **Intelligence Gathering:** Extract key information on troop movements, casualties, equipment, and political statements to enhance situational awareness.\n",
    "*   **Critical Analysis:**  Develop skills to sift through narratives and identify information potentially used for propaganda or disinformation.\n",
    "\n",
    "**Technical Setup**\n",
    "\n",
    "*   **Programming Language:**  Python (its popularity and ease of use are ideal for in-class)\n",
    "*   **Libraries:**\n",
    "    *   **Requests:** For fetching website HTML content.\n",
    "    *   **Beautiful Soup 4:**  For parsing and extracting data from HTML.\n",
    "    *   **Pandas:** For storing and manipulating scraped data (optional).\n",
    "\n",
    "**Project Outline**\n",
    "\n",
    "1.  **Website Selection**\n",
    "    *   Students research and select three news websites:\n",
    "        *   **Pro-Russian slant:**  Ex: TASS (Russian state-owned), RT, SouthFront\n",
    "        *   **Pro-Ukrainian slant:**  Ex: The Kyiv Independent, Ukrinform\n",
    "        *   **International focus:** Ex: BBC World News, Al Jazeera, Reuters\n",
    "    \n",
    "2.  **Target Elements Identification:**\n",
    "    *   **Headlines:**  Capture overall messaging\n",
    "    *   **Articles:** For in-depth analysis, identifying key events, locations, and figures.\n",
    "    *   **Dates:** To timeline events and spot evolving narratives.\n",
    "    *   **Author:** To attribute viewpoints and consider potential affiliations.\n",
    "\n",
    "3.  **Web Scraping Script Development:**\n",
    "    *   **Inspect webpage structure:** Students use browser developer tools to pinpoint the HTML tags containing the target elements.\n",
    "    *   **Write targeted Python scripts:** Employ Requests and Beautiful Soup to:\n",
    "        *   Fetch HTML from each site.\n",
    "        *   Parse the HTML, isolating target elements based on tags/classes.\n",
    "        *   Extract the text content.\n",
    "\n",
    "4.  **Data Storage and Analysis**\n",
    "    *   **Storage:** Save the results:\n",
    "        *   Simple: CSV or text files.\n",
    "        *   Advanced (optional): Use Pandas dataframes for easier manipulation.\n",
    "    *   **Analysis:** Guide students to perform:\n",
    "        *   **Comparative Word Analysis:** Word clouds, keyword frequencies to pinpoint differing terminology\n",
    "        *   **Sentiment Analysis:**  Tools to estimate positive/negative tone in the coverage. \n",
    "        *   **Timeline Visualizations:** Plotting events according to their reporting dates.\n",
    "\n",
    "**In-Class Discussion:** \n",
    "\n",
    "*   **How does each outlet frame the same event?** Look for specific word choices, omitted details, and the prominence given to particular narratives.\n",
    "*   **Identifying potential disinformation:** Does any outlet spread demonstrably false information or promote unsubstantiated claims?\n",
    "*   **IC Applications:**  How can similar techniques be used in real-world IC monitoring of evolving international situations?\n",
    "\n",
    "**Ethical Considerations**\n",
    "\n",
    "Emphasize the importance of responsible web scraping, respecting website terms of service, and avoiding excessive requests that could overload servers.\n",
    "\n",
    "**Let me know if you'd like assistance crafting the actual Python code examples or want to explore more advanced functionalities within this exercise!** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\jamal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jamal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jamal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jamal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jamal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\jamal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\jamal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in c:\\users\\jamal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\jamal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jamal\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jamal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jamal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jamal\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests\n",
    "%pip install beautifulsoup4\n",
    "%pip install pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jamal\\AppData\\Local\\Temp\\ipykernel_33496\\2947369450.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd  # Optional for advanced data handling\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd  # Optional for advanced data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Basic News Article Scraping\n",
    "\n",
    "I found articles from the requested sources that cover recent developments:\n",
    "\n",
    "#### From TASS (Pro-Russian Perspective)\n",
    "The article highlights Russia's stance on various geopolitical issues and military operations in Ukraine, including the downing of a military cargo aircraft and the engagement in the Krasny Liman direction. It covers Russia's diplomatic relations, military activities, and statements about international negotiations and alliances【[6](https://tass.com/military-operation-in-ukraine)】.\n",
    "\n",
    "#### From The Kyiv Independent (Pro-Ukrainian Perspective)\n",
    "This source provides updates on Ukraine's military and diplomatic efforts, including the attendance of President Zelensky at the Davos forum, military aid from Western allies, and Ukraine's actions against Russian military operations. It reports on the downing of Russian aircraft and the impact of Russian missile strikes on Ukrainian territories【[12](https://kyivindependent.com/tag/russias-war/)】.\n",
    "\n",
    "#### From BBC News (International Perspective)\n",
    "The BBC News article describes the severe impact of the conflict on Avdiivka, Ukraine, where after intense fighting, Russian troops have made some advances. The town, previously home to over 30,000 people, has seen its population drastically decrease, with many residents either fleeing or being killed. Despite Ukrainian efforts to defend the town, the Russians have captured several streets, leading to increased civilian evacuations. The continuous bombardment has transformed Avdiivka into a ghost town, with buildings reduced to rubble and many dead left under the debris. Volunteers and the \"White Angels\" police unit are working tirelessly to evacuate the remaining civilians, who face the dire choice of leaving their homes or facing imminent danger. 【[12](https://www.bbc.com/news/world-europe-68086568)】."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheadline\u001b[39m\u001b[38;5;124m'\u001b[39m: headline, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: article_text, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m: date}\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Example usage (replace with your actual URLs)\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m pro_russian_article \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_article\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://tass.com/world/1744761\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m pro_ukrainian_article \u001b[38;5;241m=\u001b[39m scrape_article(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://kyivindependent.com/general-staff-russia-has-lost-389-560-troops-in-ukraine/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m international_article \u001b[38;5;241m=\u001b[39m scrape_article(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.bbc.com/news/world-europe-68086568\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m, in \u001b[0;36mscrape_article\u001b[1;34m(article_url)\u001b[0m\n\u001b[0;32m      3\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# Raise an error if the request fails\u001b[39;00m\n\u001b[0;32m      5\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m headline \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mh2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnews-header\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mstrip() \n\u001b[0;32m      8\u001b[0m article_text \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnews-content\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget_text(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m date \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnews-date\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "def scrape_article(article_url):\n",
    "    response = requests.get(article_url)\n",
    "    response.raise_for_status()  # Raise an error if the request fails\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    headline = soup.find('h2', class_='news-header').text.strip() \n",
    "    article_text = soup.find('div', class_='news-content').get_text(strip=True, separator=' ')\n",
    "    date = soup.find('span', class_='news-date').text.strip()\n",
    "\n",
    "    return {'headline': headline, 'text': article_text, 'date': date}\n",
    "\n",
    "# Example usage (replace with your actual URLs)\n",
    "pro_russian_article = scrape_article('https://tass.com/world/1744761')\n",
    "pro_ukrainian_article = scrape_article('https://kyivindependent.com/general-staff-russia-has-lost-389-560-troops-in-ukraine/')\n",
    "international_article = scrape_article('https://www.bbc.com/news/world-europe-68086568')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Scraping Multiple Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news_section(section_url):\n",
    "    articles = []\n",
    "    response = requests.get(section_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    article_links = soup.find_all('a', class_='article-link')\n",
    "\n",
    "    for link in article_links:\n",
    "        article_url = link['href']\n",
    "        article_data = scrape_article(article_url)\n",
    "        articles.append(article_data)\n",
    "\n",
    "    return articles\n",
    "\n",
    "# Example\n",
    "ukraine_section_data = scrape_news_section('https://example.com/ukraine-news') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Storing Data (Optional - Using Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ukraine_section_data)\n",
    "df.to_csv('ukraine_news_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Handling Pagination\n",
    "\n",
    "    Often, news sections span multiple pages. Here's how to modify the code to navigate through them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news_section_paginated(section_url, max_pages=3):\n",
    "    articles = []\n",
    "\n",
    "    for page_num in range(1, max_pages + 1):\n",
    "        page_url = f\"{section_url}?page={page_num}\"  # Assume pagination structure\n",
    "        response = requests.get(page_url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # ... (rest of the article scraping logic from previous example) ... \n",
    "\n",
    "        articles.extend(articles)  # Combine articles from all pages\n",
    "\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Changes:\n",
    "\n",
    "Loop over pages: We introduce a loop iterating through page numbers.\n",
    "Pagination URL patterns: Adjust page_url to match how your target website handles pagination (e.g., some use '/page/2' instead of '?page=2').\n",
    "Optional max_pages: Limit scraping to a certain number of pages.\n",
    "\n",
    "6. Filtering Articles by Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article(article_url, keywords=['troops', 'equipment']):  # Add keywords\n",
    "    # ... (rest of the article scraping logic) ...\n",
    "\n",
    "    article_text = article_text.lower()  # For case-insensitive filtering\n",
    "    if any(word in article_text for word in keywords):\n",
    "        return {'headline': headline, 'text': article_text, 'date': date}\n",
    "    else:\n",
    "        return None  # Skip articles that don't match keywords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Changes:\n",
    "\n",
    "keywords parameter: The function now takes keywords to filter on.\n",
    "Filtering logic: We check if any keyword appears in the article text.\n",
    "\n",
    "7. Advanced Sentiment Analysis\n",
    "\n",
    "Let's introduce a popular sentiment analysis library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  # You might need to install: pip install nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')  # Download sentiment lexicon\n",
    "\n",
    "# Inside your analysis section:\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "for article in ukraine_section_data:\n",
    "    sentiment_scores = sia.polarity_scores(article['text'])\n",
    "    article['sentiment'] = sentiment_scores['compound']  # Store compound score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Keyword Filtering with Regular Expressions\n",
    "\n",
    "Regular expressions (regex) provide a powerful way to define complex search patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def scrape_article(article_url, keyword_regex):\n",
    "    # ... (rest of article scraping logic) ...\n",
    "\n",
    "    if re.search(keyword_regex, article_text):\n",
    "        return {'headline': headline, 'text': article_text, 'date': date}\n",
    "    else:\n",
    "        return None \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Regex Patterns:\n",
    "\n",
    "Specific equipment: r'\\btank\\b|\\bmissile\\b|\\bartillery\\b' (Word boundaries with 'or' conditions)\n",
    "Phrases: r'civilian casualties'\n",
    "Names (case-insensitive): r'(?i)putin|zelenskyy'\n",
    "Resources:\n",
    "\n",
    "Regex Tutorial: https://www.regular-expressions.info/\n",
    "Regex Tester: https://regex101.com/ (Great for building and testing your patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Visualizing Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gather sentiment scores from your scraped data\n",
    "sentiment_scores = [article['sentiment'] for article in ukraine_section_data]\n",
    "\n",
    "# Histogram of sentiment\n",
    "plt.hist(sentiment_scores)\n",
    "plt.xlabel('Sentiment Score (Compound)')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.title('Sentiment Distribution in Ukraine News Coverage')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enhancements:\n",
    "\n",
    "Seaborn: Consider the Seaborn library (https://seaborn.pydata.org/) for more aesthetically pleasing visualizations.\n",
    "Box Plots: Show distributions across different news websites to compare sentiment trends.\n",
    "Time Series: Plot average sentiment over time to see if it correlates with real-world events.\n",
    "Important Considerations\n",
    "\n",
    "Regex Complexity: Be careful – overly complex regexes can slow down your scraper.\n",
    "Visualization Interpretation: Use visualizations in conjunction with critical reading of the scraped articles. Sentiment analysis won't always capture the nuanced meaning of language.\n",
    "Further Exploration\n",
    "\n",
    "Would you be interested in any of the following?\n",
    "\n",
    "Word Clouds for Keyword Visualization: See which words are the most frequent within positively and negatively scored articles.\n",
    "Topic Modeling: Identify underlying themes within the corpus of articles (more advanced technique)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
