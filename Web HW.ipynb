{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely,  let's break down a web scraping in-class exercise tailored for an Intelligence Community (IC) perspective, focused on the Russia-Ukraine conflict.\n",
    "\n",
    "**Title:** Divergent Perspectives: Web Scraping the Russia-Ukraine Conflict\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "*   **Understanding Media Bias:** Learn how different news outlets frame the same events through selective reporting and language.\n",
    "*   **Intelligence Gathering:** Extract key information on troop movements, casualties, equipment, and political statements to enhance situational awareness.\n",
    "*   **Critical Analysis:**  Develop skills to sift through narratives and identify information potentially used for propaganda or disinformation.\n",
    "\n",
    "**Technical Setup**\n",
    "\n",
    "*   **Programming Language:**  Python (its popularity and ease of use are ideal for in-class)\n",
    "*   **Libraries:**\n",
    "    *   **Requests:** For fetching website HTML content.\n",
    "    *   **Beautiful Soup 4:**  For parsing and extracting data from HTML.\n",
    "    *   **Pandas:** For storing and manipulating scraped data (optional).\n",
    "\n",
    "**Project Outline**\n",
    "\n",
    "1.  **Website Selection**\n",
    "    *   Students research and select three news websites:\n",
    "        *   **Pro-Russian slant:**  Ex: TASS (Russian state-owned), RT, SouthFront\n",
    "        *   **Pro-Ukrainian slant:**  Ex: The Kyiv Independent, Ukrinform\n",
    "        *   **International focus:** Ex: BBC World News, Al Jazeera, Reuters\n",
    "    \n",
    "2.  **Target Elements Identification:**\n",
    "    *   **Headlines:**  Capture overall messaging\n",
    "    *   **Articles:** For in-depth analysis, identifying key events, locations, and figures.\n",
    "    *   **Dates:** To timeline events and spot evolving narratives.\n",
    "    *   **Author:** To attribute viewpoints and consider potential affiliations.\n",
    "\n",
    "3.  **Web Scraping Script Development:**\n",
    "    *   **Inspect webpage structure:** Students use browser developer tools to pinpoint the HTML tags containing the target elements.\n",
    "    *   **Write targeted Python scripts:** Employ Requests and Beautiful Soup to:\n",
    "        *   Fetch HTML from each site.\n",
    "        *   Parse the HTML, isolating target elements based on tags/classes.\n",
    "        *   Extract the text content.\n",
    "\n",
    "4.  **Data Storage and Analysis**\n",
    "    *   **Storage:** Save the results:\n",
    "        *   Simple: CSV or text files.\n",
    "        *   Advanced (optional): Use Pandas dataframes for easier manipulation.\n",
    "    *   **Analysis:** Guide students to perform:\n",
    "        *   **Comparative Word Analysis:** Word clouds, keyword frequencies to pinpoint differing terminology\n",
    "        *   **Sentiment Analysis:**  Tools to estimate positive/negative tone in the coverage. \n",
    "        *   **Timeline Visualizations:** Plotting events according to their reporting dates.\n",
    "\n",
    "**In-Class Discussion:** \n",
    "\n",
    "*   **How does each outlet frame the same event?** Look for specific word choices, omitted details, and the prominence given to particular narratives.\n",
    "*   **Identifying potential disinformation:** Does any outlet spread demonstrably false information or promote unsubstantiated claims?\n",
    "*   **IC Applications:**  How can similar techniques be used in real-world IC monitoring of evolving international situations?\n",
    "\n",
    "**Ethical Considerations**\n",
    "\n",
    "Emphasize the importance of responsible web scraping, respecting website terms of service, and avoiding excessive requests that could overload servers.\n",
    "\n",
    "**Let me know if you'd like assistance crafting the actual Python code examples or want to explore more advanced functionalities within this exercise!** \n",
    "Absolutely,  let's break down a web scraping in-class exercise tailored for an Intelligence Community (IC) perspective, focused on the Russia-Ukraine conflict.\n",
    "\n",
    "**Title:** Divergent Perspectives: Web Scraping the Russia-Ukraine Conflict\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "*   **Understanding Media Bias:** Learn how different news outlets frame the same events through selective reporting and language.\n",
    "*   **Intelligence Gathering:** Extract key information on troop movements, casualties, equipment, and political statements to enhance situational awareness.\n",
    "*   **Critical Analysis:**  Develop skills to sift through narratives and identify information potentially used for propaganda or disinformation.\n",
    "\n",
    "**Technical Setup**\n",
    "\n",
    "*   **Programming Language:**  Python (its popularity and ease of use are ideal for in-class)\n",
    "*   **Libraries:**\n",
    "    *   **Requests:** For fetching website HTML content.\n",
    "    *   **Beautiful Soup 4:**  For parsing and extracting data from HTML.\n",
    "    *   **Pandas:** For storing and manipulating scraped data (optional).\n",
    "\n",
    "**Project Outline**\n",
    "\n",
    "1.  **Website Selection**\n",
    "    *   Students research and select three news websites:\n",
    "        *   **Pro-Russian slant:**  Ex: TASS (Russian state-owned), RT, SouthFront\n",
    "        *   **Pro-Ukrainian slant:**  Ex: The Kyiv Independent, Ukrinform\n",
    "        *   **International focus:** Ex: BBC World News, Al Jazeera, Reuters\n",
    "    \n",
    "2.  **Target Elements Identification:**\n",
    "    *   **Headlines:**  Capture overall messaging\n",
    "    *   **Articles:** For in-depth analysis, identifying key events, locations, and figures.\n",
    "    *   **Dates:** To timeline events and spot evolving narratives.\n",
    "    *   **Author:** To attribute viewpoints and consider potential affiliations.\n",
    "\n",
    "3.  **Web Scraping Script Development:**\n",
    "    *   **Inspect webpage structure:** Students use browser developer tools to pinpoint the HTML tags containing the target elements.\n",
    "    *   **Write targeted Python scripts:** Employ Requests and Beautiful Soup to:\n",
    "        *   Fetch HTML from each site.\n",
    "        *   Parse the HTML, isolating target elements based on tags/classes.\n",
    "        *   Extract the text content.\n",
    "\n",
    "4.  **Data Storage and Analysis**\n",
    "    *   **Storage:** Save the results:\n",
    "        *   Simple: CSV or text files.\n",
    "        *   Advanced (optional): Use Pandas dataframes for easier manipulation.\n",
    "    *   **Analysis:** Guide students to perform:\n",
    "        *   **Comparative Word Analysis:** Word clouds, keyword frequencies to pinpoint differing terminology\n",
    "        *   **Sentiment Analysis:**  Tools to estimate positive/negative tone in the coverage. \n",
    "        *   **Timeline Visualizations:** Plotting events according to their reporting dates.\n",
    "\n",
    "**In-Class Discussion:** \n",
    "\n",
    "*   **How does each outlet frame the same event?** Look for specific word choices, omitted details, and the prominence given to particular narratives.\n",
    "*   **Identifying potential disinformation:** Does any outlet spread demonstrably false information or promote unsubstantiated claims?\n",
    "*   **IC Applications:**  How can similar techniques be used in real-world IC monitoring of evolving international situations?\n",
    "\n",
    "**Ethical Considerations**\n",
    "\n",
    "Emphasize the importance of responsible web scraping, respecting website terms of service, and avoiding excessive requests that could overload servers.\n",
    "\n",
    "**Let me know if you'd like assistance crafting the actual Python code examples or want to explore more advanced functionalities within this exercise!** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bs4'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m  \u001b[38;5;66;03m# Optional for advanced data handling\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bs4'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd  # Optional for advanced data handling\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Basic News Article Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article(article_url):\n",
    "    response = requests.get(article_url)\n",
    "    response.raise_for_status()  # Raise an error if the request fails\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    headline = soup.find('h1', class_='article-title').text.strip()  \n",
    "    article_text = soup.find('div', class_='article-body').get_text(strip=True, separator=' ')\n",
    "    date = soup.find('time').text.strip()\n",
    "\n",
    "    return {'headline': headline, 'text': article_text, 'date': date}\n",
    "\n",
    "# Example usage (replace with your actual URLs)\n",
    "pro_russian_article = scrape_article('https://tass.com')\n",
    "pro_ukrainian_article = scrape_article('https://kyivindependent.com')\n",
    "international_article = scrape_article('https://bbc.com/news')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Scraping Multiple Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news_section(section_url):\n",
    "    articles = []\n",
    "    response = requests.get(section_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    article_links = soup.find_all('a', class_='article-link')\n",
    "\n",
    "    for link in article_links:\n",
    "        article_url = link['href']\n",
    "        article_data = scrape_article(article_url)\n",
    "        articles.append(article_data)\n",
    "\n",
    "    return articles\n",
    "\n",
    "# Example\n",
    "ukraine_section_data = scrape_news_section('https://example.com/ukraine-news') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Storing Data (Optional - Using Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ukraine_section_data)\n",
    "df.to_csv('ukraine_news_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Handling Pagination\n",
    "\n",
    "    Often, news sections span multiple pages. Here's how to modify the code to navigate through them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news_section_paginated(section_url, max_pages=3):\n",
    "    articles = []\n",
    "\n",
    "    for page_num in range(1, max_pages + 1):\n",
    "        page_url = f\"{section_url}?page={page_num}\"  # Assume pagination structure\n",
    "        response = requests.get(page_url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # ... (rest of the article scraping logic from previous example) ... \n",
    "\n",
    "        articles.extend(articles)  # Combine articles from all pages\n",
    "\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Changes:\n",
    "\n",
    "Loop over pages: We introduce a loop iterating through page numbers.\n",
    "Pagination URL patterns: Adjust page_url to match how your target website handles pagination (e.g., some use '/page/2' instead of '?page=2').\n",
    "Optional max_pages: Limit scraping to a certain number of pages.\n",
    "\n",
    "6. Filtering Articles by Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article(article_url, keywords=['troops', 'equipment']):  # Add keywords\n",
    "    # ... (rest of the article scraping logic) ...\n",
    "\n",
    "    article_text = article_text.lower()  # For case-insensitive filtering\n",
    "    if any(word in article_text for word in keywords):\n",
    "        return {'headline': headline, 'text': article_text, 'date': date}\n",
    "    else:\n",
    "        return None  # Skip articles that don't match keywords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Changes:\n",
    "\n",
    "keywords parameter: The function now takes keywords to filter on.\n",
    "Filtering logic: We check if any keyword appears in the article text.\n",
    "\n",
    "7. Advanced Sentiment Analysis\n",
    "\n",
    "Let's introduce a popular sentiment analysis library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  # You might need to install: pip install nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')  # Download sentiment lexicon\n",
    "\n",
    "# Inside your analysis section:\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "for article in ukraine_section_data:\n",
    "    sentiment_scores = sia.polarity_scores(article['text'])\n",
    "    article['sentiment'] = sentiment_scores['compound']  # Store compound score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Keyword Filtering with Regular Expressions\n",
    "\n",
    "Regular expressions (regex) provide a powerful way to define complex search patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def scrape_article(article_url, keyword_regex):\n",
    "    # ... (rest of article scraping logic) ...\n",
    "\n",
    "    if re.search(keyword_regex, article_text):\n",
    "        return {'headline': headline, 'text': article_text, 'date': date}\n",
    "    else:\n",
    "        return None \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Regex Patterns:\n",
    "\n",
    "Specific equipment: r'\\btank\\b|\\bmissile\\b|\\bartillery\\b' (Word boundaries with 'or' conditions)\n",
    "Phrases: r'civilian casualties'\n",
    "Names (case-insensitive): r'(?i)putin|zelenskyy'\n",
    "Resources:\n",
    "\n",
    "Regex Tutorial: https://www.regular-expressions.info/\n",
    "Regex Tester: https://regex101.com/ (Great for building and testing your patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Visualizing Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gather sentiment scores from your scraped data\n",
    "sentiment_scores = [article['sentiment'] for article in ukraine_section_data]\n",
    "\n",
    "# Histogram of sentiment\n",
    "plt.hist(sentiment_scores)\n",
    "plt.xlabel('Sentiment Score (Compound)')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.title('Sentiment Distribution in Ukraine News Coverage')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enhancements:\n",
    "\n",
    "Seaborn: Consider the Seaborn library (https://seaborn.pydata.org/) for more aesthetically pleasing visualizations.\n",
    "Box Plots: Show distributions across different news websites to compare sentiment trends.\n",
    "Time Series: Plot average sentiment over time to see if it correlates with real-world events.\n",
    "Important Considerations\n",
    "\n",
    "Regex Complexity: Be careful â€“ overly complex regexes can slow down your scraper.\n",
    "Visualization Interpretation: Use visualizations in conjunction with critical reading of the scraped articles. Sentiment analysis won't always capture the nuanced meaning of language.\n",
    "Further Exploration\n",
    "\n",
    "Would you be interested in any of the following?\n",
    "\n",
    "Word Clouds for Keyword Visualization: See which words are the most frequent within positively and negatively scored articles.\n",
    "Topic Modeling: Identify underlying themes within the corpus of articles (more advanced technique)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
