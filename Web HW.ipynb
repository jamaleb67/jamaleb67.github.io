{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely,  let's break down a web scraping in-class exercise tailored for an Intelligence Community (IC) perspective, focused on the Russia-Ukraine conflict.\n",
    "\n",
    "**Title:** Divergent Perspectives: Web Scraping the Russia-Ukraine Conflict\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "*   **Understanding Media Bias:** Learn how different news outlets frame the same events through selective reporting and language.\n",
    "*   **Intelligence Gathering:** Extract key information on troop movements, casualties, equipment, and political statements to enhance situational awareness.\n",
    "*   **Critical Analysis:**  Develop skills to sift through narratives and identify information potentially used for propaganda or disinformation.\n",
    "\n",
    "**Technical Setup**\n",
    "\n",
    "*   **Programming Language:**  Python (its popularity and ease of use are ideal for in-class)\n",
    "*   **Libraries:**\n",
    "    *   **Requests:** For fetching website HTML content.\n",
    "    *   **Beautiful Soup 4:**  For parsing and extracting data from HTML.\n",
    "    *   **Pandas:** For storing and manipulating scraped data (optional).\n",
    "\n",
    "**Project Outline**\n",
    "\n",
    "1.  **Website Selection**\n",
    "    *   Students research and select three news websites:\n",
    "        *   **Pro-Russian slant:**  Ex: TASS (Russian state-owned), RT, SouthFront\n",
    "        *   **Pro-Ukrainian slant:**  Ex: The Kyiv Independent, Ukrinform\n",
    "        *   **International focus:** Ex: BBC World News, Al Jazeera, Reuters\n",
    "    \n",
    "2.  **Target Elements Identification:**\n",
    "    *   **Headlines:**  Capture overall messaging\n",
    "    *   **Articles:** For in-depth analysis, identifying key events, locations, and figures.\n",
    "    *   **Dates:** To timeline events and spot evolving narratives.\n",
    "    *   **Author:** To attribute viewpoints and consider potential affiliations.\n",
    "\n",
    "3.  **Web Scraping Script Development:**\n",
    "    *   **Inspect webpage structure:** Students use browser developer tools to pinpoint the HTML tags containing the target elements.\n",
    "    *   **Write targeted Python scripts:** Employ Requests and Beautiful Soup to:\n",
    "        *   Fetch HTML from each site.\n",
    "        *   Parse the HTML, isolating target elements based on tags/classes.\n",
    "        *   Extract the text content.\n",
    "\n",
    "4.  **Data Storage and Analysis**\n",
    "    *   **Storage:** Save the results:\n",
    "        *   Simple: CSV or text files.\n",
    "        *   Advanced (optional): Use Pandas dataframes for easier manipulation.\n",
    "    *   **Analysis:** Guide students to perform:\n",
    "        *   **Comparative Word Analysis:** Word clouds, keyword frequencies to pinpoint differing terminology\n",
    "        *   **Sentiment Analysis:**  Tools to estimate positive/negative tone in the coverage. \n",
    "        *   **Timeline Visualizations:** Plotting events according to their reporting dates.\n",
    "\n",
    "**In-Class Discussion:** \n",
    "\n",
    "*   **How does each outlet frame the same event?** Look for specific word choices, omitted details, and the prominence given to particular narratives.\n",
    "*   **Identifying potential disinformation:** Does any outlet spread demonstrably false information or promote unsubstantiated claims?\n",
    "*   **IC Applications:**  How can similar techniques be used in real-world IC monitoring of evolving international situations?\n",
    "\n",
    "**Ethical Considerations**\n",
    "\n",
    "Emphasize the importance of responsible web scraping, respecting website terms of service, and avoiding excessive requests that could overload servers.\n",
    "\n",
    "**Let me know if you'd like assistance crafting the actual Python code examples or want to explore more advanced functionalities within this exercise!** \n",
    "Absolutely,  let's break down a web scraping in-class exercise tailored for an Intelligence Community (IC) perspective, focused on the Russia-Ukraine conflict.\n",
    "\n",
    "**Title:** Divergent Perspectives: Web Scraping the Russia-Ukraine Conflict\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "*   **Understanding Media Bias:** Learn how different news outlets frame the same events through selective reporting and language.\n",
    "*   **Intelligence Gathering:** Extract key information on troop movements, casualties, equipment, and political statements to enhance situational awareness.\n",
    "*   **Critical Analysis:**  Develop skills to sift through narratives and identify information potentially used for propaganda or disinformation.\n",
    "\n",
    "**Technical Setup**\n",
    "\n",
    "*   **Programming Language:**  Python (its popularity and ease of use are ideal for in-class)\n",
    "*   **Libraries:**\n",
    "    *   **Requests:** For fetching website HTML content.\n",
    "    *   **Beautiful Soup 4:**  For parsing and extracting data from HTML.\n",
    "    *   **Pandas:** For storing and manipulating scraped data (optional).\n",
    "\n",
    "**Project Outline**\n",
    "\n",
    "1.  **Website Selection**\n",
    "    *   Students research and select three news websites:\n",
    "        *   **Pro-Russian slant:**  Ex: TASS (Russian state-owned), RT, SouthFront\n",
    "        *   **Pro-Ukrainian slant:**  Ex: The Kyiv Independent, Ukrinform\n",
    "        *   **International focus:** Ex: BBC World News, Al Jazeera, Reuters\n",
    "    \n",
    "2.  **Target Elements Identification:**\n",
    "    *   **Headlines:**  Capture overall messaging\n",
    "    *   **Articles:** For in-depth analysis, identifying key events, locations, and figures.\n",
    "    *   **Dates:** To timeline events and spot evolving narratives.\n",
    "    *   **Author:** To attribute viewpoints and consider potential affiliations.\n",
    "\n",
    "3.  **Web Scraping Script Development:**\n",
    "    *   **Inspect webpage structure:** Students use browser developer tools to pinpoint the HTML tags containing the target elements.\n",
    "    *   **Write targeted Python scripts:** Employ Requests and Beautiful Soup to:\n",
    "        *   Fetch HTML from each site.\n",
    "        *   Parse the HTML, isolating target elements based on tags/classes.\n",
    "        *   Extract the text content.\n",
    "\n",
    "4.  **Data Storage and Analysis**\n",
    "    *   **Storage:** Save the results:\n",
    "        *   Simple: CSV or text files.\n",
    "        *   Advanced (optional): Use Pandas dataframes for easier manipulation.\n",
    "    *   **Analysis:** Guide students to perform:\n",
    "        *   **Comparative Word Analysis:** Word clouds, keyword frequencies to pinpoint differing terminology\n",
    "        *   **Sentiment Analysis:**  Tools to estimate positive/negative tone in the coverage. \n",
    "        *   **Timeline Visualizations:** Plotting events according to their reporting dates.\n",
    "\n",
    "**In-Class Discussion:** \n",
    "\n",
    "*   **How does each outlet frame the same event?** Look for specific word choices, omitted details, and the prominence given to particular narratives.\n",
    "*   **Identifying potential disinformation:** Does any outlet spread demonstrably false information or promote unsubstantiated claims?\n",
    "*   **IC Applications:**  How can similar techniques be used in real-world IC monitoring of evolving international situations?\n",
    "\n",
    "**Ethical Considerations**\n",
    "\n",
    "Emphasize the importance of responsible web scraping, respecting website terms of service, and avoiding excessive requests that could overload servers.\n",
    "\n",
    "**Let me know if you'd like assistance crafting the actual Python code examples or want to explore more advanced functionalities within this exercise!** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd  # Optional for advanced data handling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Basic News Article Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article(article_url):\n",
    "    response = requests.get(article_url)\n",
    "    response.raise_for_status()  # Raise an error if the request fails\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    headline = soup.find('h1', class_='article-title').text.strip()  \n",
    "    article_text = soup.find('div', class_='article-body').get_text(strip=True, separator=' ')\n",
    "    date = soup.find('time').text.strip()\n",
    "\n",
    "    return {'headline': headline, 'text': article_text, 'date': date}\n",
    "\n",
    "# Example usage (replace with your actual URLs)\n",
    "pro_russian_article = scrape_article('https://example.com/pro-russian-article')\n",
    "pro_ukrainian_article = scrape_article('https://example.com/pro-ukrainian-article')\n",
    "international_article = scrape_article('https://example.com/international-article')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Scraping Multiple Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news_section(section_url):\n",
    "    articles = []\n",
    "    response = requests.get(section_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    article_links = soup.find_all('a', class_='article-link')\n",
    "\n",
    "    for link in article_links:\n",
    "        article_url = link['href']\n",
    "        article_data = scrape_article(article_url)\n",
    "        articles.append(article_data)\n",
    "\n",
    "    return articles\n",
    "\n",
    "# Example\n",
    "ukraine_section_data = scrape_news_section('https://example.com/ukraine-news') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Storing Data (Optional - Using Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ukraine_section_data)\n",
    "df.to_csv('ukraine_news_data.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
