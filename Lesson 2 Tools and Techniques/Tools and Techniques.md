# Tools and Techniques
**Background**

- **Techniques** are set of **procedures** followed to perform a task
- **Tools** are **resources** used to apply techniques to perform a task
- Data scientists:
    - Apply **operational methods** called **techniques** on data through various **resources** (usually software)
  called **Tools**  
    - Use the combinationof tools and techniques to:
        - Acquire data
        - Manipulate and label data
        - Examine results for best possible outcomes. 

**Data Cleaning, Wrangling and Munging**
  - Data sets are characterized by:
    - missing data
    - duplicate data
    - outliers
    that can distort any efforts to determine trends, patterns, and ultimately trustworthy decisions on the data.

Data cleaning, wrangling and munging are often misrepresented for though they have some overlap, there are some unique
and required differences to be observed in terms of their context

In **data cleaning**, the emphasis is on removing inaccurate data from the data set.
In **data wrangling**, the focus is on transforming the data's format from typically a "raw" data set into a more suitable format
**Data munging and data wrangling **are viewed in the same context.

There are three type of statistical and analytical techniques most widely used by data scientists

In **data processing**, data is converted into a useable form making it more meaningful and informative.

In **data mining**, interesting and potential useful knowledge is extracted from massive amounts of data 

Data mining supports:
  - Accelerating the pace of making informed decisions
  - Sifting through all the chaotic and repetitive noise in data
  - Understanding what is relevant and then making good use of that information to assess likely outcomes.

**Data modeling**
  - A "roadmap" for analyzing data objects and their relationships to other objects
  - Used to analyze data requirements for business processes
  - Describes the structures, flows, mappings, transformations, relationships and quality of data
  - Employs standardized "schema" and formal techniques which yield a "blueprint" for defining and managing data resources
    across and beyond an organization. 

**Data collection and Normalization of Ground Truths**
  - Ground truth data refers to data collected at scale from real-world scenarios
    - "What you see, is what you get"
  - Ground truth data is important because the effectiveness of machine learning and artificial intelligence rely on the quality of data
    - However, the capture of ground truth is not easy because thee are no universal standards for data capture
    - Collected data is only relevant to the particular environment and relevant circumstances.

**Data Collection and IC Challenges**
  - The **single most important step** in solving problems is **data collection**, which consists of data acquisition, data entry, data labeling, and data extraction
  - Many also view **data collection** as one of the **most critical roadblocks** for users and data scientists

**IC Challenges** 
  - _Detection_
    - As high counterintelligence (CI) threat areas proliferate, officers struggle to maintain anonymity
  - _Denial_
    - AI-enabled advances in crytography and cybersecurity helps adversaries "harden" and encrypt their systems
  - _Degrading_
  
  Hostile foreign intelligence services could exploit AI to put technical platforms under persistent threat
 
 **Other Tools**<br>
  **Web scraping tools**
    - Web pages written in HTML, which is unstructured and non well annotated and requires external libraries<br>
  **Web crawling tools** 
    - Type of bot typically operated by search engines to index the content of websites all across the Internet so that those websites can appear in search engine results

**Application Programming Interfaces (APIs)**
  - Interfaces in which data can bee collected off of different sites (often social media sites) for empirical analysis
  - Advantages
    - Processes can be automated
    - Can allow the explicit request of data in a structured format
  - Disadvantage
    -  Do not provide unlimited access to entire databases
  
   - Machine learning algorithms
   - Neural networks
   - Social Network Analysis tools
  Limitations
   - Algorithmic Limitations
   - Cultural hesitance
      - From analysis standpoint, there is consistent concern about human remaining in the loop or on the loop.

  **Data Science Environment**
  - Data Science is constantly evolving and requiring an environment that supports a great deal of updatingg and thus collaboration
  - It is important to be able to track changes in sets of files and be able to engage in projects that range from the small to the very large
  - Git is a free, open source distributed version control system designed to handle such projects with speed and efficiency
  - One of the most prominent cloud-based hosting platforms to manage Git repositories is GitHub/GitLab, which are designed to help better manage Git
    - GitHub/Gitlab is basically a tool for tracking changes made in files. 



