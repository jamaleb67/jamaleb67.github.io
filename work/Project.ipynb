{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Youtube/GDP](Top)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://i.imgur.com/ACFFpv2.png width=1500 class=\"center\">\n",
    "<h1 align=\"center\">Top Youtubers effecting GDP between 2019 - 2022??</h1>\n",
    "    \n",
    "    In today's world, the internet has become an integral part of our lives. With the rise of online platforms such as YouTube, it has become easier than ever for people to access information and entertainment from all over the world. At the same time, Gross Domestic Product (GDP) remains one of the most widely used indicators of economic performance. In this project, we aim to explore the relationship between these two seemingly unrelated topics: GDP and YouTube. By analyzing data on GDP and YouTube usage patterns, we hope to gain insights into how these two factors are connected and what implications this may have on our understanding of the modern world.\n",
    "\n",
    "The objective of this project is to analyze the relationship between GDP and YouTube usage patterns. We will use data on GDP and YouTube usage patterns to gain insights into how these two factors are connected and what implications this may have on our understanding of the modern world.\n",
    "\n",
    "Our goal is to answer the following questions:\n",
    "\n",
    "- What are the the top Youtubers in countries around the globe?\n",
    "- What is the Top GDP countries, and what is there growth during COVID?\n",
    "- Is there a correlation between Top Youtubers and selected GDP Nations?\n",
    "- What is statistical corelations can be made?\n",
    "\n",
    "To answer these questions, we will use Python and its data analysis libraries, such as Pandas and Matplotlib. We will start by importing the dataset and cleaning the data, followed by exploratory data analysis and visualization.\n",
    "\n",
    "I will be using the following datasets: \n",
    "- [Top Youtubers](https://www.kaggle.com/mdhrumil/top-5000-youtube-channels-data-from-socialblade)\n",
    "- [GDP](https://www.kaggle.com/fernandol/countries-of-the-world)\n",
    "\n",
    "APIs:\n",
    "- [YouTube API](https://developers.google.com/youtube/v3/docs/channels/list)\n",
    "- [Google API](https://console.cloud.google.com/apis/library/youtube.googleapis.com)\n",
    "\n",
    "\n",
    "\n",
    "# Table of contents <a class='anchor' id='top'>\n",
    "- [Introduction](#Introduction)\n",
    "- [Import libraries](#import)\n",
    "- [Load data](#load_data)\n",
    "- [GDP Analysis](#gdpproject)\n",
    "- [Bar chart](#bar_chart)\n",
    "- [GDP Conclusion](#geo)\n",
    "- [YouTube Analysis](#Analysis)\n",
    "- [Youtube API](#YouTube)\n",
    "- [Conclusion](#Conclusion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction  <a class='anchor' id='Introduction'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries <a class='anchor' id='import'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install isodate\n",
    "%pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client\n",
    "%pip install wordcloud\n",
    "%pip install nltk\n",
    "%pip install wbgapi\n",
    "%pip install bar_chart_race\n",
    "%pip install plotly\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install seaborn\n",
    "%pip install numpy\n",
    "%pip install requests\n",
    "%pip install scipy\n",
    "%pip install sklearn\n",
    "%pip install statsmodels\n",
    "%pip install xgboost\n",
    "%pip install lightgbm\n",
    "%pip install catboost\n",
    "%pip install bar_chart_race\n",
    "%pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil import parser\n",
    "import isodate\n",
    "\n",
    "# Data visualization libraries\n",
    "%pip install scikit-learn\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import pyplot as pyplot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "sns.set(style=\"darkgrid\", color_codes=True)\n",
    "%pip install scikit-learn\n",
    "\n",
    "# Google API\n",
    "from googleapiclient.discovery import build\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP libraries\n",
    "import collections\n",
    "import itertools\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import ImageColorGenerator\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <b><u><span style=\"font-size: 24px\">\n",
    "  GDP Analysis from 2019 - 2020<a class='anchor' id='gdpproject'></span></u></b><br>\n",
    "</p>\n",
    "\n",
    "### Load data <a class='anchor' id='load_data'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "#load data\n",
    "data = pd.read_csv('GDP by Country 1999-2022.csv', decimal = ',')\n",
    "\n",
    "data = data.replace({'\\\"' :''}, regex=True)\n",
    "data = data.replace({',' :''}, regex=True)\n",
    "#display(data)\n",
    "\n",
    "data = data.astype({'1999' : 'float', '2022' : 'float'})\n",
    "\n",
    "#observe data\n",
    "data.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar Chart for Top 10 GDP countries and for targeted group <a class='anchor' id='bar_chart'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 10 GDP in 2022\n",
    "top_10_GDP_2022 = data.sort_values('2022', ascending = False).head(10)\n",
    "fig, ax = pyplot.subplots(figsize = (16, 6))\n",
    "sns.barplot(x = 'Country', y = '2022', data = top_10_GDP_2022, palette = 'Set1')\n",
    "ax.set_xlabel(ax.get_xlabel(), labelpad= 15)\n",
    "ax.set_ylabel('Gross Domestic Product', labelpad= 30)\n",
    "ax.xaxis.label.set_fontsize(16)\n",
    "ax.yaxis.label.set_fontsize(16)\n",
    "pyplot.xticks(rotation = 90)\n",
    "pyplot.title('Top GDP producing Countries in 2022', fontsize = 20)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2022 GDP for germany, south korea, UK, US, Mexico, japan and india\n",
    "\n",
    "Countries = data[data[\"Country\"].isin(['Germany', 'South Korea', 'United Kingdom', 'United States', 'Mexico', 'Japan', 'India'])] \n",
    "Countries_GDP_2022 = Countries.sort_values('2022', ascending = False)\n",
    "\n",
    "display(Countries_GDP_2022)\n",
    "\n",
    "fig, ax = pyplot.subplots(figsize = (16, 6))\n",
    "sns.barplot(x = 'Country', y = '2022', data = Countries_GDP_2022, legend=False, palette = 'Set1')\n",
    "ax.set_xlabel(ax.get_xlabel(), labelpad= 15)\n",
    "ax.set_ylabel('GDP by Country 2022', labelpad= 30)\n",
    "ax.xaxis.label.set_fontsize(16)\n",
    "ax.yaxis.label.set_fontsize(16)\n",
    "pyplot.xticks(rotation = 45)\n",
    "pyplot.title('2022 GDP for Germany, South korea, UK, US, Mexico, Japan and India', fontsize = 20)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Gross Domestic product average for 2019 - 2020 <a class='anchor' id='19-20'>\n",
    "+ Germany, South Korea UK, US, Mexico, Japan and India"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average GDP between 2018-2022 for germany, south korea, UK, US, Mexico, japan and india\n",
    "Countries = data[data[\"Country\"].isin(['Germany', 'South Korea', 'United Kingdom', 'United States', 'Mexico', 'Japan', 'India'])] \n",
    "Countries_GDP_2022 = Countries[['Country', '2018', '2019', '2020', '2021', '2022']]\n",
    "\n",
    "fig, ax = pyplot.subplots(figsize = (16, 6))\n",
    "new_data = pd.melt(Countries_GDP_2022, id_vars = ['Country'], value_vars = ['2018', '2019', '2020', '2021', '2022'], var_name = 'Year', value_name = 'GDP')\n",
    "\n",
    "new_data = new_data.astype({'Country' : 'str'})\n",
    "new_data = new_data.astype({'GDP' : 'float'})\n",
    "\n",
    "\n",
    "display(new_data)\n",
    "\n",
    "sns.pointplot(data = new_data, x = 'Year', y = 'GDP', hue = 'Country', ax = ax, palette = 'nipy_spectral')\n",
    "\n",
    "ax.set_xlabel(ax.get_xlabel(), labelpad= 15)\n",
    "ax.set_ylabel('Average GDP by Country 2018-2022', labelpad= 30)\n",
    "ax.xaxis.label.set_fontsize(16)\n",
    "ax.yaxis.label.set_fontsize(16)\n",
    "pyplot.title('Average GDP between 2018-2022', fontsize = 20)\n",
    "pyplot.show()\n",
    "\n",
    "#sort_new_data = new_data.sort_values('GDP', ascending = True)\n",
    "#sort_new_data = resort_new_data.sort_values('GDP', ascending = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#sort_new_data = new_data.sort_values('GDP', ascending = True)\n",
    "#sort_new_data = resort_new_data.sort_values('GDP', ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average GDP between 2018-2022 for germany, south korea, UK, Mexico, japan and india ***Without the US because it made the graph look bad***\n",
    "Countries = data[data[\"Country\"].isin(['Germany', 'South Korea', 'United Kingdom', 'Mexico', 'Japan', 'India'])] \n",
    "Countries_GDP_2022 = Countries[['Country', '2018', '2019', '2020', '2021', '2022']]\n",
    "\n",
    "fig, ax = pyplot.subplots(figsize = (16, 8))\n",
    "new_data = pd.melt(Countries_GDP_2022, id_vars = ['Country'], value_vars = ['2018', '2019', '2020', '2021', '2022'], var_name = 'Year', value_name = 'GDP')\n",
    "\n",
    "new_data = new_data.astype({'Country' : 'str'})\n",
    "new_data = new_data.astype({'GDP' : 'float'})\n",
    "\n",
    "\n",
    "\n",
    "display(new_data)\n",
    "\n",
    "sns.pointplot(data = new_data, x = 'Year', y = 'GDP', hue = 'Country', ax = ax, palette = 'nipy_spectral')\n",
    "\n",
    "ax.set_xlabel(ax.get_xlabel(), labelpad= 15)\n",
    "ax.set_ylabel('Average GDP by Country 2018-2022', labelpad= 30)\n",
    "ax.xaxis.label.set_fontsize(16)\n",
    "ax.yaxis.label.set_fontsize(16)\n",
    "pyplot.title('Average GDP between 2018-2022 without the U.S.', fontsize = 20)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HalfTime <a class='anchor' id='geo'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Region Code: You must provide a two-letter ISO 3166–1 country code \n",
    "* (e.g., ‘US’ for the United States) to specify the region for which you want to find the top 10 YouTube channels.\n",
    "* Replace 'YOUR_REGION_CODE' in the code with the desired region code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ?WBGAPI World Bank Top 20 Countries by GDP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import wbgapi as wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(wb)\n",
    "wb.source.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##wb.economy.info(db=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## WORKING CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "class Coder(dict):\n",
    "    '''Class returned by coder if passed a list of terms\n",
    "    '''\n",
    "\n",
    "    def __repr__(self):\n",
    "        rows = self._coder_report()\n",
    "        columns = rows.pop(0)\n",
    "        return tabulate(rows, tablefmt='simple', headers=columns)\n",
    "\n",
    "def coder_report(economies):\n",
    "\n",
    "    global _coder_names\n",
    "\n",
    "    rows = [('ORIGINAL NAME', 'WBG NAME', 'ISO_CODE')]\n",
    "    for k,v in economies.items():\n",
    "        if v:\n",
    "            wb_name = _coder_names.get(v, '')\n",
    "        else:\n",
    "            wb_name = ''\n",
    "\n",
    "        rows.append((k, wb_name, v))\n",
    "\n",
    "    output = []\n",
    "    for row in rows:\n",
    "        output.append([row[0], row[1], row[2]])\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "def plot_anim_funct():\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.set_xlim((0, 2))\n",
    "    ax.set_ylim((-2, 2))\n",
    "\n",
    "    line, = ax.plot([], [], lw=2)\n",
    "\n",
    "    def init():\n",
    "        line.set_data([], [])\n",
    "        return (line,)\n",
    "\n",
    "    def animate(i):\n",
    "        x = np.linspace(0, 2, 1000)\n",
    "        y = np.sin(2 * np.pi * (x - 0.01 * i))\n",
    "        line.set_data(x, y)\n",
    "        return (line,)\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                                   frames=100, interval=20, \n",
    "                                   blit=True)\n",
    "\n",
    "\n",
    "    display(HTML(anim.to_jshtml()))\n",
    "\n",
    "plot_anim_funct()\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"GDP by Country 1999-2022.csv\", sep=',', header=0, thousands=\",\")\n",
    "df.set_index(\"Country\", inplace=True)\n",
    "df = df.T\n",
    "print(f\"Dataframe has {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML, display\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import base64\n",
    "from io import BytesIO, TextIOWrapper\n",
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "from matplotlib import rcParams\n",
    "from matplotlib import animation\n",
    "\n",
    "class FuncAnimation(animation.FuncAnimation):\n",
    "\n",
    "    def to_html5_video(self, embed_limit=None, savefig_kwargs=None):\n",
    "        \"\"\"\n",
    "        Convert the animation to an HTML5 ``<video>`` tag.\n",
    "\n",
    "        This saves the animation as an h264 video, encoded in base64\n",
    "        directly into the HTML5 video tag. This respects the rc parameters\n",
    "        for the writer as well as the bitrate. This also makes use of the\n",
    "        ``interval`` to control the speed, and uses the ``repeat``\n",
    "        parameter to decide whether to loop.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        embed_limit : float, optional\n",
    "            Limit, in MB, of the returned animation. No animation is created\n",
    "            if the limit is exceeded.\n",
    "            Defaults to :rc:`animation.embed_limit` = 20.0.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        video_tag : str\n",
    "            An HTML5 video tag with the animation embedded as base64 encoded\n",
    "            h264 video.\n",
    "            If the *embed_limit* is exceeded, this returns the string\n",
    "            \"Video too large to embed.\"\n",
    "        \"\"\"\n",
    "        VIDEO_TAG = r'''<video {size} {options}>\n",
    "  <source type=\"video/mp4\" src=\"data:video/mp4;base64,{video}\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>'''\n",
    "        # Cache the rendering of the video as HTML\n",
    "        if not hasattr(self, '_base64_video'):\n",
    "            # Save embed limit, which is given in MB\n",
    "            if embed_limit is None:\n",
    "                embed_limit = rcParams['animation.embed_limit']\n",
    "\n",
    "            # Convert from MB to bytes\n",
    "            embed_limit *= 1024 * 1024\n",
    "\n",
    "            # Can't open a NamedTemporaryFile twice on Windows, so use a\n",
    "            # TemporaryDirectory instead.\n",
    "            with TemporaryDirectory() as tmpdir:\n",
    "                path = Path(tmpdir, \"temp.m4v\")\n",
    "                # We create a writer manually so that we can get the\n",
    "                # appropriate size for the tag\n",
    "                Writer = animation.writers[rcParams['animation.writer']]\n",
    "                writer = Writer(codec='h264',\n",
    "                                bitrate=rcParams['animation.bitrate'],\n",
    "                                fps=1000. / self._interval)\n",
    "                self.save(str(path), writer=writer, savefig_kwargs=savefig_kwargs)\n",
    "                # Now open and base64 encode.\n",
    "                vid64 = base64.encodebytes(path.read_bytes())\n",
    "\n",
    "            vid_len = len(vid64)\n",
    "            if vid_len >= embed_limit:\n",
    "                _log.warning(\n",
    "                    \"Animation movie is %s bytes, exceeding the limit of %s. \"\n",
    "                    \"If you're sure you want a large animation embedded, set \"\n",
    "                    \"the animation.embed_limit rc parameter to a larger value \"\n",
    "                    \"(in MB).\", vid_len, embed_limit)\n",
    "            else:\n",
    "                self._base64_video = vid64.decode('ascii')\n",
    "                self._video_size = 'width=\"{}\" height=\"{}\"'.format(\n",
    "                        *writer.frame_size)\n",
    "\n",
    "        # If we exceeded the size, this attribute won't exist\n",
    "        if hasattr(self, '_base64_video'):\n",
    "            # Default HTML5 options are to autoplay and display video controls\n",
    "            options = ['controls', 'autoplay']\n",
    "\n",
    "            # If we're set to repeat, make it loop\n",
    "            if hasattr(self, 'repeat') and self.repeat:\n",
    "                options.append('loop')\n",
    "\n",
    "            return VIDEO_TAG.format(video=self._base64_video,\n",
    "                                    size=self._video_size,\n",
    "                                    options=' '.join(options))\n",
    "        else:\n",
    "            return 'Video too large to embed.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import bar_chart_race as bcr\n",
    "import warnings\n",
    "import matplotlib.animation\n",
    "import imageio_ffmpeg as ffmpeg\n",
    "\n",
    "---\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "bcr.bar_chart_race(df=df,\n",
    "                   n_bars=10,\n",
    "                  orientation=\"h\",\n",
    "                  title=\"Gross Domestic Product (billions USD)\",\n",
    "                  cmap=\"tab20b\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import bar_chart_race as bcr\n",
    "\n",
    "create a dataframe with sample data\n",
    "set the index to 'Year'\n",
    "df.set_index('Year', inplace=True)\n",
    "\n",
    "create the bar chart race\n",
    "bcr.bar_chart_race(df=df, n_bars=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube Exploratory Data Analysis <a class='anchor' id='Analysis'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare for Youtube API pull and research of top YouTubers, we need to first obtain the API key from Google Cloud Console. Once we have the API key, we can use it to authenticate our requests to the Youtube API. We can then use the API to pull data on top YouTubers, such as their subscriber count, view count, and video count. We can use this data to perform exploratory data analysis and gain insights into the trends and patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U.S. Top Channels\n",
    "\n",
    "\n",
    "1. Mr. Beasts view-source:https://www.youtube.com/@MrBeast/about\n",
    "* UCX6OQ3DkcsbYNE6H8uQQuVA\n",
    "\n",
    "2. Cocomelon - Nursery Rhymes view-source:https://www.youtube.com/channel/UCbCmjCuTUZos6Inko4u57UQ\n",
    "* UCbCmjCuTUZos6Inko4u57UQ\n",
    "\n",
    "3. Dude Perfect view-source:https://www.youtube.com/channel/UCRijo3ddMTht_IHyNSNXpNQ\n",
    "* UCRijo3ddMTht_IHyNSNXpNQ\n",
    "\n",
    "4. ✿ Kids Diana Show view-source:https://www.youtube.com/channel/UCk8GzjMOrta8yxDcKfylJYw\n",
    "* UCk8GzjMOrta8yxDcKfylJYw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For India, the top two YouTubers are **Bhuvaneshwar Bam** and **Amit Bhadana**. Bhuvaneshwar Bam is the creator of **BB ki Vines**, a comedy channel that features him playing multiple characters. He has more than 20 million subscribers and over 3 billion views. Amit Bhadana is another comedy channel that features slice of life content, relationships, and comedic skits. He has more than 22 million subscribers and over 1.8 billion views. You can visit their YouTube pages here: [BB ki Vines](^1^) and [Amit Bhadana](^2^).\n",
    "\n",
    "- For Japan, the top two YouTubers are **HikakinTV** and **Yuka Kinoshita**. HikakinTV is a channel that features various content such as music, games, challenges, and collaborations with other celebrities. He has more than 8.8 million subscribers and over 6.7 billion views. Yuka Kinoshita is a channel that features her eating large amounts of food in a short time. She has more than 5.6 million subscribers and over 2.4 billion views. You can visit their YouTube pages here: [HikakinTV](^3^) and [Yuka Kinoshita].\n",
    "\n",
    "- For Mexico, the top two YouTubers are **Luisito Comunica** and **Badabun**. Luisito Comunica is a channel that features his travels around the world, exploring different cultures, cuisines, and attractions. He has more than 36 million subscribers and over 4.9 billion views. Badabun is a channel that features various content such as entertainment, news, pranks, and social experiments. He has more than 43 million subscribers and over 14 billion views. You can visit their YouTube pages here: [Luisito Comunica] and [Badabun].\n",
    "\n",
    "- For South Korea, the top two YouTubers are **Boram Tube Vlog** and **Saebyuk Jang**. Boram Tube Vlog is a channel that features a six-year-old girl named Boram and her family doing various activities such as playing with toys, cooking, traveling, and reviewing products. She has more than 26 million subscribers and over 10 billion views. Saebyuk Jang is a channel that features a young boy named Saebyuk and his parents doing various content such as games, challenges, vlogs, and animations. He has more than 23 million subscribers and over 8 billion views. You can visit their YouTube pages here: [Boram Tube Vlog] and [Saebyuk Jang].\n",
    "\n",
    "\n",
    "1) YouTube. https://www.youtube.com/index.\n",
    "2) Top 50 Popular YouTubers in India (2023) - Moneymint. https://moneymint.com/top-youtubers-in-india/.\n",
    "3) List of most-subscribed YouTube channels - Wikipedia. https://en.wikipedia.org/wiki/List_of_most-subscribed_YouTube_channels.\n",
    "#### India Top Channels\n",
    "1.  view-source:\n",
    "*\n",
    "2.   view-source:\n",
    "*\n",
    "\n",
    "#### Japan Top Channels\n",
    "1.  view-source:\n",
    "*\n",
    "2.  view-source:\n",
    "*\n",
    "\n",
    "#### Mexico Top Channels\n",
    "1.  view-source:\n",
    "*\n",
    "2.  view-source:\n",
    "*\n",
    "\n",
    "#### South Korea Top Channels\n",
    "1.  view-source:\n",
    "*\n",
    "2.  view-source:\n",
    "*\n",
    "\n",
    "#### United Kingdom Top Channels\n",
    "1.  view-source:\n",
    "*\n",
    "2.  view-source:\n",
    "*\n",
    "\n",
    "#### Germany Top Channels\n",
    "1.\n",
    "*\n",
    "2.\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YouTube API <a class='anchor' id='YouTube'></center> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data creation with Youtube API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'AIzaSyDs9TExsfc8fDMn4lBRYXZax1vSr0ftsdk'  # Youtube API key Personal \n",
    "\n",
    "channel_ids = ['UCX6OQ3DkcsbYNE6H8uQQuVA', # Mr. Beast US\n",
    "               'UCbCmjCuTUZos6Inko4u57UQ', # Cocomelon US\n",
    "               'UCqwUrj10mAEsqezcItqvwEw', # Bhuvaneshwar Bam IN\n",
    "               'UC_vcKmg67vjMP7ciLnSxSHQ', # Amit Bhadana IN\n",
    "               'UCZf__ehlCEBPop-_sldpBUQ', # HikakinTV JP\n",
    "               'UC1opHUrw8rvnsadT-iGp7Cg', # PewDiePie SE\n",
    "               'UCYWOjHweP2V-8kGKmmAmQJQ', # Badabun MX \n",
    "               'UCECJDeK0MNapZbpaOzxrUPA', # Luisito Comunica MX\n",
    "               'UCOmHUn--16B90oW2L6FRR3A', # BLACKPINK\n",
    "              ]\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Function to pull data from Youtube API and create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_stats(youtube, channel_ids):\n",
    "    \"\"\"\n",
    "    Get channel statistics: title, subscriber count, view count, video count, upload playlist\n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    channels_ids: list of channel IDs\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe containing the channel statistics for all channels in the provided list: title, subscriber count, view count, video count, upload playlist\n",
    "    \n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    request = youtube.channels().list(\n",
    "                part='snippet,contentDetails,statistics',\n",
    "                id=','.join(channel_ids))\n",
    "    response = request.execute() \n",
    "    \n",
    "    for i in range(len(response['items'])):\n",
    "        data = dict(channelName = response['items'][i]['snippet']['title'],\n",
    "                    subscribers = response['items'][i]['statistics']['subscriberCount'],\n",
    "                    views = response['items'][i]['statistics']['viewCount'],\n",
    "                    totalVideos = response['items'][i]['statistics']['videoCount'],\n",
    "                    playlistId = response['items'][i]['contentDetails']['relatedPlaylists']['uploads'])\n",
    "        all_data.append(data)\n",
    "    \n",
    "    return pd.DataFrame(all_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New method to pull data from Youtube API and create a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_video_ids(youtube, playlist_id):\n",
    "    \"\"\"\n",
    "    Get list of video IDs of all videos in the given playlist\n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    playlist_id: playlist ID of the channel\n",
    "    \n",
    "    Returns:\n",
    "    List of video IDs of all videos in the playlist\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    request = youtube.playlistItems().list(\n",
    "                part='contentDetails',\n",
    "                playlistId = playlist_id,\n",
    "                maxResults = 50)\n",
    "    response = request.execute()\n",
    "    \n",
    "    video_ids = []\n",
    "\n",
    "        for i in range(len(response['items'])):\n",
    "        video_ids.append(response['items'][i]['contentDetails']['videoId'])\n",
    "        \n",
    "    next_page_token = response.get('nextPageToken')\n",
    "    more_pages = True\n",
    "    \n",
    "    while more_pages:\n",
    "        if next_page_token is None:\n",
    "            more_pages = False\n",
    "        else:\n",
    "            request = youtube.playlistItems().list(\n",
    "                        part='contentDetails',\n",
    "                        playlistId = playlist_id,\n",
    "                        maxResults = 50,\n",
    "                        pageToken = next_page_token)\n",
    "            response = request.execute()\n",
    "    \n",
    "            for i in range(len(response['items'])):\n",
    "                video_ids.append(response['items'][i]['contentDetails']['videoId'])\n",
    "            \n",
    "            next_page_token = response.get('nextPageToken')\n",
    "        \n",
    "    return video_ids\n",
    "def get_comments_in_videos(youtube, video_ids):\n",
    "    \"\"\"\n",
    "    Get the top 10 comments for each provided video ID.\n",
    "    \n",
    "    Parameters:\n",
    "    youtube (googleapiclient.discovery.Resource): The YouTube API resource object.\n",
    "    video_ids (list): List of video IDs.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing video IDs and associated top-level comments in text.\n",
    "    \"\"\"\n",
    "    all_comments = []\n",
    "    for video_id in video_ids:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=10  # Fetch only the first 10 comments\n",
    "            )\n",
    "            response = request.execute()\n",
    "            # Process response data and append to all_comments\n",
    "            # ...\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching comments for video {video_id}: {e}\")\n",
    "            continue\n",
    "    return pd.DataFrame(all_comments)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  channelName subscribers         views totalVideos  \\\n",
      "0            Luisito Comunica    41400000    9051059451        1316   \n",
      "1                 BB Ki Vines    26300000    4825068409         190   \n",
      "2  Cocomelon - Nursery Rhymes   167000000  170365245506        1027   \n",
      "3                     MrBeast   207000000   36268184336         765   \n",
      "4                   HikakinTV    11900000   11709391701        3368   \n",
      "\n",
      "                 playlistId  \n",
      "0  UUECJDeK0MNapZbpaOzxrUPA  \n",
      "1  UUqwUrj10mAEsqezcItqvwEw  \n",
      "2  UUbCmjCuTUZos6Inko4u57UQ  \n",
      "3  UUX6OQ3DkcsbYNE6H8uQQuVA  \n",
      "4  UUZf__ehlCEBPop-_sldpBUQ  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='youtube_data.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize YouTube API client\n",
    "# Replace 'YOUR_API_KEY' with your actual API key\n",
    "youtube = build('youtube', 'v3', developerKey='AIzaSyDs9TExsfc8fDMn4lBRYXZax1vSr0ftsdk')\n",
    "\n",
    "def get_channel_stats(youtube, channel_ids):\n",
    "    \"\"\"\n",
    "    Get channel statistics: title, subscriber count, view count, video count, upload playlist\n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    channels_ids: list of channel IDs\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe containing the channel statistics for all channels in the provided list: title, subscriber count, view count, video count, upload playlist\n",
    "    \n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    request = youtube.channels().list(\n",
    "                part='snippet,contentDetails,statistics',\n",
    "                id=','.join(channel_ids))\n",
    "    response = request.execute() \n",
    "    \n",
    "    for i in range(len(response['items'])):\n",
    "        data = dict(channelName = response['items'][i]['snippet']['title'],\n",
    "                    subscribers = response['items'][i]['statistics']['subscriberCount'],\n",
    "                    views = response['items'][i]['statistics']['viewCount'],\n",
    "                    totalVideos = response['items'][i]['statistics']['videoCount'],\n",
    "                    playlistId = response['items'][i]['contentDetails']['relatedPlaylists']['uploads'])\n",
    "        all_data.append(data)\n",
    "    \n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "\n",
    "def get_top_video_ids_by_view_count(youtube, playlist_id):\n",
    "    \"\"\"\n",
    "    #Get the top 10 video IDs from the given playlist sorted by view count!\n",
    "    # Fetch video IDs from the playlist\n",
    "    # ...\n",
    "    # Sort video IDs by view count and return top 10\n",
    "    # ...\n",
    "    Params:\n",
    "    youtube (googleapiclient.discovery.Resource): The YouTube API resource object.\n",
    "    playlist_id (str): Playlist ID.\n",
    "    \n",
    "    Returns:\n",
    "    List of the top 10 video IDs in the playlist sorted by view count.\n",
    "    \"\"\"\n",
    "    request = youtube.playlistItems().list(\n",
    "        part='contentDetails',\n",
    "        playlistId=playlist_id,\n",
    "        maxResults=50\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    # Extract video IDs and view counts into a list of dictionaries\n",
    "    videos_data = [{'video_id': item['contentDetails']['videoId']} for item in response.get('items', [])]\n",
    "\n",
    "    # If there are more pages, continue fetching\n",
    "    next_page_token = response.get('nextPageToken')\n",
    "    while next_page_token and len(videos_data) < 10:\n",
    "        request = youtube.playlistItems().list(\n",
    "            part='contentDetails',\n",
    "            playlistId=playlist_id,\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "        videos_data.extend([{'video_id': item['contentDetails']['videoId']} for item in response.get('items', [])])\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "\n",
    "    # Sort the videos by view count in descending order and return the top 10\n",
    "    top_video_ids = [video['video_id'] for video in videos_data]\n",
    "    return top_video_ids[:10]\n",
    "\n",
    "\n",
    "def get_video_details(youtube, video_ids):\n",
    "    \"\"\"\n",
    "    Get details for the provided video IDs.\n",
    "    \n",
    "    Parameters:\n",
    "    youtube (googleapiclient.discovery.Resource): The YouTube API resource object.\n",
    "    video_ids (list): List of video IDs.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing video details.\n",
    "    \"\"\"\n",
    "    all_video_info = []\n",
    "    for video_id in video_ids:\n",
    "        try:\n",
    "            request = youtube.videos().list(\n",
    "                part=\"snippet,contentDetails,statistics\",\n",
    "                id=video_id\n",
    "            )\n",
    "            response = request.execute()\n",
    "            # Process response data and append to all_video_info\n",
    "            video_info = response['items'][0]\n",
    "            snippet = video_info['snippet']\n",
    "            statistics = video_info['statistics']\n",
    "            content_details = video_info['contentDetails']\n",
    "            \n",
    "            video_data = {\n",
    "                'video_id': video_id,\n",
    "                'channelTitle': snippet.get('channelTitle', ''),\n",
    "                'title': snippet.get('title', ''),\n",
    "                'description': snippet.get('description', ''),\n",
    "                'tags': snippet.get('tags', []),\n",
    "                'publishedAt': snippet.get('publishedAt', ''),\n",
    "                'viewCount': statistics.get('viewCount', 0),\n",
    "                'likeCount': statistics.get('likeCount', 0),\n",
    "                'favoriteCount': statistics.get('favoriteCount', 0),\n",
    "                'commentCount': statistics.get('commentCount', 0),\n",
    "                'duration': content_details.get('duration', ''),\n",
    "                'definition': content_details.get('definition', ''),\n",
    "                'caption': content_details.get('caption', ''),\n",
    "            }\n",
    "            \n",
    "            all_video_info.append(video_data)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching details for video {video_id}: {e}\")\n",
    "            continue\n",
    "    return pd.DataFrame(all_video_info)\n",
    "\n",
    "def get_top_liked_comments(youtube, video_ids, max_results=10, min_likes=10):\n",
    "    \"\"\"\n",
    "    Get the top liked comments for each provided video ID and filter out spam-like comments.\n",
    "\n",
    "    Parameters:\n",
    "    youtube (googleapiclient.discovery.Resource): The YouTube API resource object.\n",
    "    video_ids (list): List of video IDs.\n",
    "    max_results (int): Maximum number of comments to retrieve for each video (default is 10).\n",
    "    min_likes (int): Minimum number of likes for a comment to be considered non-spam (default is 10).\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing video IDs and associated top liked comments.\n",
    "    \"\"\"\n",
    "    all_comments = []\n",
    "    for video_id in video_ids:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=max_results,  # Fetch the specified number of comments\n",
    "                order=\"relevance\",  # Order comments by relevance (likely to include top liked comments)\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            # Process comments and filter out spam-like comments\n",
    "            comments_in_video = []\n",
    "            for item in response['items']:\n",
    "                snippet = item['snippet']['topLevelComment']['snippet']\n",
    "                like_count = snippet.get('likeCount', 0)\n",
    "                comment_text = snippet.get('textDisplay', '')\n",
    "\n",
    "                # Filter out spam-like comments based on the minimum likes threshold\n",
    "                if like_count >= min_likes:\n",
    "                    comments_in_video.append(comment_text)\n",
    "\n",
    "            comments_in_video_info = {'video_id': video_id,'comments': comments_in_video}\n",
    "            all_comments.append(comments_in_video_info)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching comments for video {video_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(all_comments)\n",
    "\n",
    "\n",
    "def get_comments_in_videos(youtube, video_ids, max_results=10, min_likes=50):\n",
    "    \"\"\"\n",
    "    # ... (Implementation of get_top_liked_comments function)\n",
    "    Get the top 10 comments for each provided video ID.\n",
    "\n",
    "    Parameters:\n",
    "    youtube (googleapiclient.discovery.Resource): The YouTube API resource object.\n",
    "    video_ids (list): List of video IDs.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing video IDs and associated top-level comments in text.\n",
    "    \"\"\"\n",
    "    all_comments = []\n",
    "    for video_id in video_ids:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=max_results,  # Fetch only the first 10 comments #UPDATED to Fetch the specified number of comments\n",
    "                order=\"relevance\",  # Order comments by relevance (likely to include top liked comments) \n",
    "            )\n",
    "            response = request.execute()\n",
    "            # Process response data and append to all_comments # UPDATED to Process comments and filter out spam-like comments\n",
    "            # Process comments and filter out spam-like comments\n",
    "            for item in response['items']:\n",
    "                snippet = item['snippet']['topLevelComment']['snippet']\n",
    "                comment_data = {\n",
    "                    'video_id': video_id,\n",
    "                    'comment_id': item['id'],\n",
    "                    'author': snippet.get('authorDisplayName', ''),\n",
    "                    'text': snippet.get('textDisplay', ''),\n",
    "                    'like_count': snippet.get('likeCount', 0),\n",
    "                    'published_at': snippet.get('publishedAt', ''),\n",
    "                }\n",
    "                all_comments.append(comment_data)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching comments for video {video_id}: {e}\")\n",
    "            continue\n",
    "    return pd.DataFrame(all_comments)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # List of channel IDs to process\n",
    "    channel_ids = ['UCX6OQ3DkcsbYNE6H8uQQuVA','UCbCmjCuTUZos6Inko4u57UQ','UCqwUrj10mAEsqezcItqvwEw','UC_vcKmg67vjMP7ciLnSxSHQ','UCZf__ehlCEBPop-_sldpBUQ','UC1opHUrw8rvnsadT-iGp7Cg', 'UCYWOjHweP2V-8kGKmmAmQJQ','UCECJDeK0MNapZbpaOzxrUPA','UCOmHUn--16B90oW2L6FRR3A']\n",
    "    \n",
    "    # Get channel statistics\n",
    "    channel_data = get_channel_stats(youtube, channel_ids)\n",
    "    \n",
    "    # Print the first few rows of channel_data to inspect column names\n",
    "    print(channel_data.head())\n",
    "    \n",
    "    video_df = pd.DataFrame()\n",
    "    comments_df = pd.DataFrame()\n",
    "    \n",
    "    # Ensure the column name matches the actual column name in channel_data\n",
    "    for channel in channel_data['channelName'].unique():\n",
    "        logging.info(f\"Processing channel: {channel}\")\n",
    "        playlist_id = channel_data.loc[channel_data['channelName'] == channel, 'playlistId'].iloc[0]\n",
    "        try:\n",
    "            video_ids = get_top_video_ids_by_view_count(youtube, playlist_id)\n",
    "            video_data = get_video_details(youtube, video_ids)\n",
    "            comments_data = get_comments_in_videos(youtube, video_ids)\n",
    "\n",
    "            video_df = pd.concat([video_df, video_data]).drop_duplicates(subset=['video_id'])\n",
    "            comments_df = pd.concat([comments_df, comments_data]).drop_duplicates(subset=['video_id'])\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing channel {channel}: {e}\")\n",
    "            continue\n",
    "        \n",
    "    # Save the data to CSV files\n",
    "    video_df.to_csv('video_data.csv', index=False)\n",
    "    comments_df.to_csv('comments_data.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "               'UC1opHUrw8rvnsadT-iGp7Cg', # PewDiePie SE\n",
    "               'UCYWOjHweP2V-8kGKmmAmQJQ', # Badabun MX \n",
    "               'UCECJDeK0MNapZbpaOzxrUPA', # Luisito Comunica MX\n",
    "               'UCOmHUn--16B90oW2L6FRR3A', # BLACKPINK\n",
    "\n",
    "def main():\n",
    "    # List of channel IDs to process \n",
    "    channel_ids = ['UCX6OQ3DkcsbYNE6H8uQQuVA', # Mr. Beast US\n",
    "               'UCbCmjCuTUZos6Inko4u57UQ', # Cocomelon US\n",
    "               'UCqwUrj10mAEsqezcItqvwEw', # Bhuvaneshwar Bam IN\n",
    "               'UC_vcKmg67vjMP7ciLnSxSHQ', # Amit Bhadana IN\n",
    "               'UCZf__ehlCEBPop-_sldpBUQ', # HikakinTV JP\n",
    "              ]  # Replace with actual channel IDs\n",
    "    \n",
    "    # Get channel statistics\n",
    "    channel_data = get_channel_stats(youtube, channel_ids)\n",
    "    \n",
    "    video_df = pd.DataFrame()\n",
    "    comments_df = pd.DataFrame()\n",
    "    \n",
    "    for channel in channel_data['channelName'].unique():\n",
    "        logging.info(f\"Processing channel: {channel}\")\n",
    "        playlist_id = channel_data.loc[channel_data['channelName'] == channel, 'playlistId'].iloc[0]\n",
    "        try:\n",
    "            video_ids = get_top_video_ids_by_view_count(youtube, playlist_id)\n",
    "            video_data = get_video_details(youtube, video_ids)\n",
    "            comments_data = get_comments_in_videos(youtube, video_ids)\n",
    "\n",
    "            video_df = pd.concat([video_df, video_data]).drop_duplicates(subset=['video_id'])\n",
    "            comments_df = pd.concat([comments_df, comments_data]).drop_duplicates(subset=['video_id'])\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing channel {channel}: {e}\")\n",
    "            continue\n",
    "        \n",
    "    # Save the data to CSV files\n",
    "    video_df.to_csv('video_data.csv', index=False)\n",
    "    comments_df.to_csv('comments_data.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_top_video_ids(youtube, playlist_id):\n",
    "    \"\"\"\n",
    "    Get the top 10 video IDs from the given playlist sorted by view count.\n",
    "    \n",
    "    Params:\n",
    "    youtube (googleapiclient.discovery.Resource): The YouTube API resource object.\n",
    "    playlist_id (str): Playlist ID.\n",
    "    \n",
    "    Returns:\n",
    "    List of the top 10 video IDs in the playlist sorted by view count.\n",
    "    \"\"\"\n",
    "    request = youtube.playlistItems().list(\n",
    "        part='contentDetails',\n",
    "        playlistId=playlist_id,\n",
    "        maxResults=50\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    # Extract video IDs and view counts into a list of dictionaries\n",
    "    videos_data = [{'video_id': item['contentDetails']['videoId']} for item in response.get('items', [])]\n",
    "\n",
    "    # If there are more pages, continue fetching\n",
    "    next_page_token = response.get('nextPageToken')\n",
    "    while next_page_token and len(videos_data) < 10:\n",
    "        request = youtube.playlistItems().list(\n",
    "            part='contentDetails',\n",
    "            playlistId=playlist_id,\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "        videos_data.extend([{'video_id': item['contentDetails']['videoId']} for item in response.get('items', [])])\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "\n",
    "    # Sort the videos by view count in descending order and return the top 10\n",
    "    top_video_ids = [video['video_id'] for video in videos_data]\n",
    "    return top_video_ids[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "def get_video_details(youtube, video_ids):\n",
    "    \"\"\"\n",
    "    Get video statistics of all videos with given IDs\n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    video_ids: list of video IDs\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe with statistics of videos, i.e.:\n",
    "        'channelTitle', 'title', 'description', 'tags', 'publishedAt'\n",
    "        'viewCount', 'likeCount', 'favoriteCount', 'commentCount'\n",
    "        'duration', 'definition', 'caption'\n",
    "    \"\"\"\n",
    "        \n",
    "    all_video_info = []\n",
    "    \n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet,contentDetails,statistics\",\n",
    "            id=','.join(video_ids[i:i+50])\n",
    "        )\n",
    "        response = request.execute() \n",
    "\n",
    "        for video in response['items']:\n",
    "            stats_to_keep = {'snippet': ['channelTitle', 'title', 'description', 'tags', 'publishedAt'],\n",
    "                             'statistics': ['viewCount', 'likeCount', 'favouriteCount', 'commentCount'],\n",
    "                             'contentDetails': ['duration', 'definition', 'caption']\n",
    "                            }\n",
    "            video_info = {}\n",
    "            video_info['video_id'] = video['id']\n",
    "\n",
    "            for k in stats_to_keep.keys():\n",
    "                for v in stats_to_keep[k]:\n",
    "                    try:\n",
    "                        video_info[v] = video[k][v]\n",
    "                    except:\n",
    "                        video_info[v] = None\n",
    "\n",
    "            all_video_info.append(video_info)\n",
    "            \n",
    "    return pd.DataFrame(all_video_info)\n",
    "\n",
    "def get_comments_in_videos(youtube, video_ids):\n",
    "    \"\"\"\n",
    "    Get top level comments as text from all videos with given IDs (only the first 10 comments due to quote limit of Youtube API)\n",
    "    Params:\n",
    "    \n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    video_ids: list of video IDs\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe with video IDs and associated top level comment in text.\n",
    "    \n",
    "    \"\"\"\n",
    "    all_comments = []\n",
    "    \n",
    "    for video_id in video_ids:\n",
    "        try:   \n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet,replies\",\n",
    "                videoId=video_id\n",
    "            )\n",
    "            response = request.execute()\n",
    "        \n",
    "            comments_in_video = [comment['snippet']['topLevelComment']['snippet']['textOriginal'] for comment in response['items'][0:10]]\n",
    "            comments_in_video_info = {'video_id': video_id, 'comments': comments_in_video}\n",
    "\n",
    "            all_comments.append(comments_in_video_info)\n",
    "            \n",
    "        except: \n",
    "            # When error occurs - most likely because comments are disabled on a video\n",
    "            print('Could not get comments for video ' + video_id)\n",
    "        \n",
    "    return pd.DataFrame(all_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Get channel statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `get_channel_stats` function defined below, now we are going to obtain the channel statistics for the 9 channels in scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_stats(youtube, channel_ids):\n",
    "    \"\"\"\n",
    "    Get channel statistics: title, subscriber count, view count, video count, upload playlist, and country.\n",
    "    \n",
    "    Parameters:\n",
    "    youtube: the build object from googleapiclient.discovery\n",
    "    channels_ids: list of channel IDs\n",
    "    \n",
    "    Returns:\n",
    "    Dataframe containing the channel statistics for all channels in the provided list: title, subscriber count, view count, video count, upload playlist, and country.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    for channel_id in channel_ids:\n",
    "        try:\n",
    "            request = youtube.channels().list(\n",
    "                part='snippet,contentDetails,statistics',\n",
    "                id=channel_id\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            # Fetch additional data including country\n",
    "            snippet_request = youtube.channels().list(\n",
    "                part='snippet',\n",
    "                id=channel_id\n",
    "            )\n",
    "            snippet_response = snippet_request.execute()\n",
    "            country = snippet_response['items'][0]['snippet'].get('country', '')\n",
    "\n",
    "            data = dict(\n",
    "                channelName=response['items'][0]['snippet']['title'],\n",
    "                subscribers=response['items'][0]['statistics']['subscriberCount'],\n",
    "                views=response['items'][0]['statistics']['viewCount'],\n",
    "                totalVideos=response['items'][0]['statistics']['videoCount'],\n",
    "                playlistId=response['items'][0]['contentDetails']['relatedPlaylists']['uploads'],\n",
    "                country=country  # Add country information\n",
    "            )\n",
    "            all_data.append(data)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error fetching channel stats for {channel_id}: {e}\")\n",
    "            continue\n",
    "    return pd.DataFrame(all_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'channelName': 'MrBeast',\n",
       "  'subscribers': '207000000',\n",
       "  'views': '36268184336',\n",
       "  'totalVideos': '765',\n",
       "  'playlistId': 'UUX6OQ3DkcsbYNE6H8uQQuVA',\n",
       "  'country': 'US'},\n",
       " {'channelName': 'Cocomelon - Nursery Rhymes',\n",
       "  'subscribers': '167000000',\n",
       "  'views': '170365245506',\n",
       "  'totalVideos': '1027',\n",
       "  'playlistId': 'UUbCmjCuTUZos6Inko4u57UQ',\n",
       "  'country': 'US'},\n",
       " {'channelName': 'BB Ki Vines',\n",
       "  'subscribers': '26300000',\n",
       "  'views': '4825068409',\n",
       "  'totalVideos': '190',\n",
       "  'playlistId': 'UUqwUrj10mAEsqezcItqvwEw',\n",
       "  'country': 'IN'},\n",
       " {'channelName': 'Amit Bhadana',\n",
       "  'subscribers': '24500000',\n",
       "  'views': '2429214564',\n",
       "  'totalVideos': '106',\n",
       "  'playlistId': 'UU_vcKmg67vjMP7ciLnSxSHQ',\n",
       "  'country': 'IN'},\n",
       " {'channelName': 'HikakinTV',\n",
       "  'subscribers': '11900000',\n",
       "  'views': '11709391701',\n",
       "  'totalVideos': '3368',\n",
       "  'playlistId': 'UUZf__ehlCEBPop-_sldpBUQ',\n",
       "  'country': ''},\n",
       " {'channelName': 'Aqua Ch. 湊あくあ',\n",
       "  'subscribers': '1920000',\n",
       "  'views': '368194153',\n",
       "  'totalVideos': '592',\n",
       "  'playlistId': 'UU1opHUrw8rvnsadT-iGp7Cg',\n",
       "  'country': 'JP'},\n",
       " {'channelName': 'Badabun',\n",
       "  'subscribers': '47100000',\n",
       "  'views': '19554408537',\n",
       "  'totalVideos': '19518',\n",
       "  'playlistId': 'UUYWOjHweP2V-8kGKmmAmQJQ',\n",
       "  'country': 'MX'},\n",
       " {'channelName': 'Luisito Comunica',\n",
       "  'subscribers': '41400000',\n",
       "  'views': '9051059451',\n",
       "  'totalVideos': '1316',\n",
       "  'playlistId': 'UUECJDeK0MNapZbpaOzxrUPA',\n",
       "  'country': 'MX'},\n",
       " {'channelName': 'BLACKPINK',\n",
       "  'subscribers': '92000000',\n",
       "  'views': '33904663784',\n",
       "  'totalVideos': '585',\n",
       "  'playlistId': 'UUOmHUn--16B90oW2L6FRR3A',\n",
       "  'country': 'KR'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channelName</th>\n",
       "      <th>subscribers</th>\n",
       "      <th>views</th>\n",
       "      <th>totalVideos</th>\n",
       "      <th>playlistId</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MrBeast</td>\n",
       "      <td>207000000</td>\n",
       "      <td>36268184336</td>\n",
       "      <td>765</td>\n",
       "      <td>UUX6OQ3DkcsbYNE6H8uQQuVA</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cocomelon - Nursery Rhymes</td>\n",
       "      <td>167000000</td>\n",
       "      <td>170365245506</td>\n",
       "      <td>1027</td>\n",
       "      <td>UUbCmjCuTUZos6Inko4u57UQ</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BB Ki Vines</td>\n",
       "      <td>26300000</td>\n",
       "      <td>4825068409</td>\n",
       "      <td>190</td>\n",
       "      <td>UUqwUrj10mAEsqezcItqvwEw</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amit Bhadana</td>\n",
       "      <td>24500000</td>\n",
       "      <td>2429214564</td>\n",
       "      <td>106</td>\n",
       "      <td>UU_vcKmg67vjMP7ciLnSxSHQ</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HikakinTV</td>\n",
       "      <td>11900000</td>\n",
       "      <td>11709391701</td>\n",
       "      <td>3368</td>\n",
       "      <td>UUZf__ehlCEBPop-_sldpBUQ</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  channelName subscribers         views totalVideos  \\\n",
       "0                     MrBeast   207000000   36268184336         765   \n",
       "1  Cocomelon - Nursery Rhymes   167000000  170365245506        1027   \n",
       "2                 BB Ki Vines    26300000    4825068409         190   \n",
       "3                Amit Bhadana    24500000    2429214564         106   \n",
       "4                   HikakinTV    11900000   11709391701        3368   \n",
       "\n",
       "                 playlistId country  \n",
       "0  UUX6OQ3DkcsbYNE6H8uQQuVA      US  \n",
       "1  UUbCmjCuTUZos6Inko4u57UQ      US  \n",
       "2  UUqwUrj10mAEsqezcItqvwEw      IN  \n",
       "3  UU_vcKmg67vjMP7ciLnSxSHQ      IN  \n",
       "4  UUZf__ehlCEBPop-_sldpBUQ          "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = []\n",
    "    \n",
    "for channel_id in channel_ids:\n",
    "    try:\n",
    "        request = youtube.channels().list(\n",
    "            part='snippet,contentDetails,statistics',\n",
    "            id=channel_id\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        # Fetch additional data including country\n",
    "        snippet_request = youtube.channels().list(\n",
    "            part='snippet',\n",
    "            id=channel_id\n",
    "        )\n",
    "        snippet_response = snippet_request.execute()\n",
    "        country = snippet_response['items'][0]['snippet'].get('country', '')\n",
    "\n",
    "        data = dict(\n",
    "            channelName=response['items'][0]['snippet']['title'],\n",
    "            subscribers=response['items'][0]['statistics']['subscriberCount'],\n",
    "            views=response['items'][0]['statistics']['viewCount'],\n",
    "            totalVideos=response['items'][0]['statistics']['videoCount'],\n",
    "            playlistId=response['items'][0]['contentDetails']['relatedPlaylists']['uploads'],\n",
    "            country=country  # Add country information\n",
    "        )\n",
    "        all_data.append(data)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching channel stats for {channel_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "channel_data1 = pd.DataFrame(all_data)\n",
    "channel_data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_data1.to_csv('Top9channel_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert count columns to numeric columns\n",
    "numeric_cols = ['subscribers', 'views', 'totalVideos']\n",
    "channel_data[numeric_cols] = channel_data[numeric_cols].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channelName</th>\n",
       "      <th>subscribers</th>\n",
       "      <th>views</th>\n",
       "      <th>totalVideos</th>\n",
       "      <th>playlistId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BLACKPINK</td>\n",
       "      <td>92000000</td>\n",
       "      <td>33904663784</td>\n",
       "      <td>585</td>\n",
       "      <td>UUOmHUn--16B90oW2L6FRR3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HikakinTV</td>\n",
       "      <td>11900000</td>\n",
       "      <td>11709391701</td>\n",
       "      <td>3368</td>\n",
       "      <td>UUZf__ehlCEBPop-_sldpBUQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aqua Ch. 湊あくあ</td>\n",
       "      <td>1920000</td>\n",
       "      <td>368194153</td>\n",
       "      <td>592</td>\n",
       "      <td>UU1opHUrw8rvnsadT-iGp7Cg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Badabun</td>\n",
       "      <td>47100000</td>\n",
       "      <td>19554408537</td>\n",
       "      <td>19517</td>\n",
       "      <td>UUYWOjHweP2V-8kGKmmAmQJQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Luisito Comunica</td>\n",
       "      <td>41400000</td>\n",
       "      <td>9051059451</td>\n",
       "      <td>1316</td>\n",
       "      <td>UUECJDeK0MNapZbpaOzxrUPA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cocomelon - Nursery Rhymes</td>\n",
       "      <td>167000000</td>\n",
       "      <td>170365245506</td>\n",
       "      <td>1027</td>\n",
       "      <td>UUbCmjCuTUZos6Inko4u57UQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MrBeast</td>\n",
       "      <td>207000000</td>\n",
       "      <td>36268184336</td>\n",
       "      <td>765</td>\n",
       "      <td>UUX6OQ3DkcsbYNE6H8uQQuVA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Amit Bhadana</td>\n",
       "      <td>24500000</td>\n",
       "      <td>2429214564</td>\n",
       "      <td>106</td>\n",
       "      <td>UU_vcKmg67vjMP7ciLnSxSHQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BB Ki Vines</td>\n",
       "      <td>26300000</td>\n",
       "      <td>4825068409</td>\n",
       "      <td>190</td>\n",
       "      <td>UUqwUrj10mAEsqezcItqvwEw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  channelName  subscribers         views  totalVideos  \\\n",
       "0                   BLACKPINK     92000000   33904663784          585   \n",
       "1                   HikakinTV     11900000   11709391701         3368   \n",
       "2               Aqua Ch. 湊あくあ      1920000     368194153          592   \n",
       "3                     Badabun     47100000   19554408537        19517   \n",
       "4            Luisito Comunica     41400000    9051059451         1316   \n",
       "5  Cocomelon - Nursery Rhymes    167000000  170365245506         1027   \n",
       "6                     MrBeast    207000000   36268184336          765   \n",
       "7                Amit Bhadana     24500000    2429214564          106   \n",
       "8                 BB Ki Vines     26300000    4825068409          190   \n",
       "\n",
       "                 playlistId  \n",
       "0  UUOmHUn--16B90oW2L6FRR3A  \n",
       "1  UUZf__ehlCEBPop-_sldpBUQ  \n",
       "2  UU1opHUrw8rvnsadT-iGp7Cg  \n",
       "3  UUYWOjHweP2V-8kGKmmAmQJQ  \n",
       "4  UUECJDeK0MNapZbpaOzxrUPA  \n",
       "5  UUbCmjCuTUZos6Inko4u57UQ  \n",
       "6  UUX6OQ3DkcsbYNE6H8uQQuVA  \n",
       "7  UU_vcKmg67vjMP7ciLnSxSHQ  \n",
       "8  UUqwUrj10mAEsqezcItqvwEw  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_data.to_csv('channel_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top Viewership ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Using Channel_data dataframe from previous step, the data is sorted, and the top 9 channels are plotted with a legend for clarity.\n",
    "\n",
    "# Sort the data and get the top 9 channels for clarity in the legend\n",
    "sorted_data = channel_data.sort_values('subscribers', ascending=False)[0:9]\n",
    "\n",
    "# Create a color palette with a distinct color for each bar\n",
    "palette = sns.color_palette(\"hsv\", len(sorted_data))\n",
    "\n",
    "# Create the bar plot with the specified palette\n",
    "ax = sns.barplot(\n",
    "    x='channelName', \n",
    "    y='subscribers', \n",
    "    data=sorted_data, \n",
    "    palette=palette\n",
    ")\n",
    "\n",
    "# Rotate the x-axis labels by 45 degrees\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Format the y-axis labels as thousands with a K suffix\n",
    "ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: f'{x/1000:.0f}K'))\n",
    "\n",
    "# Remove x-axis labels as we will add these as legends\n",
    "ax.set(xticklabels=[])\n",
    "\n",
    "# Create the legend manually\n",
    "for i, row in enumerate(sorted_data.itertuples()):\n",
    "    plt.bar(0, 0, color=palette[i], label=row.channelName,)\n",
    "\n",
    "# Place the legend below the chart # Adjust legend box height and width\n",
    "plt.legend(title='The Top YouTuber Channel Names', bbox_to_anchor=(1.35, .5), loc='center')\n",
    "\n",
    "# Add labels to the bars to show the exact subscriber count for each channel\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x() + p.get_width() / 2., height + 0.02 * height, \n",
    "            f'{height/1000:.0f}K', ha='center', va='bottom', fontsize=8, color='black'\n",
    "            )\n",
    "\n",
    "#  Add a title and format it\n",
    "plt.title('Top 9 YouTuber Channels by Subscriber Count', x=0.7, y=1.15, loc='Center', fontsize=18, fontweight='bold')\n",
    "\n",
    "\n",
    "# FILEPATH: /c:/Users/jamal/jamaleb67.github.io/jamaleb67.github.io/work/Project.ipynb\n",
    "plt.suptitle('Source: YouTube API', x=0.05, y=0.02, ha='left', fontsize=7)\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig(\"TOP9PLOT.png\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "ax = sns.barplot(x='channelName', y='views', data=channel_data.sort_values('views', ascending=False))\n",
    "ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))\n",
    "plot = ax.set_xticklabels(ax.get_xticklabels(),rotation = 90)\n",
    "\n",
    "\n",
    "# Using Channel_data dataframe from previous step, the data is sorted, and the top 9 channels are plotted with a legend for clarity.\n",
    "\n",
    "# Sort the data and get the top 9 channels for clarity in the legend\n",
    "sorted_data = channel_data.sort_values('views', ascending=False)[0:9]\n",
    "\n",
    "# Create a color palette with a distinct color for each bar\n",
    "palette = sns.color_palette(\"hsv\", len(sorted_data))\n",
    "\n",
    "# Create the bar plot with the specified palette\n",
    "ax = sns.barplot(\n",
    "    x='channelName', \n",
    "    y='views', \n",
    "    data=sorted_data, \n",
    "    palette=palette\n",
    ")\n",
    "\n",
    "# Rotate the x-axis labels by 45 degrees\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Format the y-axis labels as thousands with a K suffix\n",
    "ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: f'{x/1000:.0f}K'))\n",
    "\n",
    "# Remove x-axis labels as we will add these as legends\n",
    "ax.set(xticklabels=[])\n",
    "\n",
    "# Create the legend manually\n",
    "for i, row in enumerate(sorted_data.itertuples()):\n",
    "    plt.bar(0, 0, color=palette[i], label=row.channelName,)\n",
    "\n",
    "# Place the legend below the chart # Adjust legend box height and width\n",
    "plt.legend(title='The Top YouTuber Channels by Viewership', bbox_to_anchor=(1.35, .5), loc='center')\n",
    "\n",
    "# Add labels to the bars to show the exact subscriber count for each channel\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x() + p.get_width() / 2., height + 0.02 * height, \n",
    "            f'{height/1000:.0f}K', ha='center', va='bottom', rotation=10,  fontsize=7\n",
    "            )\n",
    "\n",
    "#  Add a title and format it\n",
    "plt.title('Top 9 YouTuber Channels by View Count', x=0.7, y=1.15, loc='Center', fontsize=18, fontweight='bold')\n",
    "\n",
    "\n",
    "# FILEPATH: /c:/Users/jamal/jamaleb67.github.io/jamaleb67.github.io/work/Project.ipynb\n",
    "plt.suptitle('Source: YouTube API', x=0.05, y=0.02, ha='left', fontsize=8)\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig(\"TOP9PLOT.png\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Video Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video statistics are obtained using the `get_video_stats` function defined below. The function takes in a list of video ids and returns a dataframe with the video statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_df = pd.DataFrame()\n",
    "comments_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(video_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataframe with video statistics and comments from all channels\n",
    "\n",
    "#video_df = pd.DataFrame()\n",
    "#comments_df = pd.DataFrame() \n",
    "\n",
    "for c in channel_data['channelName'].unique():\n",
    "    print(\"Getting video information from channel: \" + c)\n",
    "    playlist_id = channel_data.loc[channel_data['channelName']== c, 'playlistId'].iloc[0]\n",
    "    video_ids = get_video_ids(youtube, playlist_id)\n",
    "    \n",
    "    # get video data\n",
    "    video_data = get_video_details(youtube, video_ids)\n",
    "    # get comment data\n",
    "    comments_data = get_comments_in_videos(youtube, video_ids)\n",
    "\n",
    "    # append video data together and comment data toghether\n",
    "    video_df = video_df.concat(video_data, ignore_index=True)\n",
    "    comments_df = comments_df.concat(comments_data, ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataframe with video statistics and comments from all channels\n",
    "import pandas as pd\n",
    "\n",
    "video_df = pd.DataFrame()\n",
    "comments_df = pd.DataFrame()\n",
    "\n",
    "for c in channel_data['channelName'].unique():\n",
    "    print(\"Getting video information from channel: \" + c)\n",
    "    playlist_id = channel_data.loc[channel_data['channelName']== c, 'playlistId'].iloc[0]\n",
    "    video_ids = get_video_ids(youtube, playlist_id)\n",
    "    \n",
    "    # get video data\n",
    "    video_data = get_video_details(youtube, video_ids)\n",
    "\n",
    "    # append video data together and comment data toghether\n",
    "    video_df = video_df.append(video_data, ignore_index=True)\n",
    "    comments_df = comments_df.concat(comments_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "video_df = pd.DataFrame()\n",
    "\n",
    "for c in channel_data['channelName'].unique():\n",
    "    print(\"Getting video information from channel: \" + c)\n",
    "    playlist_id = channel_data.loc[channel_data['channelName'] == c, 'playlistId'].iloc[0]\n",
    "    video_ids = get_video_ids(youtube, playlist_id)\n",
    "    \n",
    "    # get video data\n",
    "    video_data = get_video_details(youtube, video_ids)\n",
    "\n",
    "    # append video data together\n",
    "    video_df = pd.concat([video_df, video_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write video data to CSV file for future references\n",
    "video_df.to_csv('video_data_top10_channels.csv')\n",
    "comments_df.to_csv('comments_data_top10_channels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed a Power BI report in a Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion <a class='anchor' id='Conclusion'></a>[↑](#Top)>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Reference\n",
    "\n",
    "#### Get all items\n",
    "\n",
    "```http\n",
    "  GET /api/items\n",
    "```\n",
    "\n",
    "| Parameter | Type     | Description                |\n",
    "| :-------- | :------- | :------------------------- |\n",
    "| `api_key` | `string` | **AIzaSyDs9TExsfc8fDMn4lBRYXZax1vSr0ftsdk**. YouTube API key |\n",
    "\n",
    "#### Get item\n",
    "\n",
    "```http\n",
    "  GET /api/items/${id}\n",
    "```\n",
    "\n",
    "| Parameter | Type     | Description                       |\n",
    "| :-------- | :------- | :-------------------------------- |\n",
    "| `id`      | `string` | **UCtYLUTtgS3k1Fg4y5tAhLbw** # Statquest\n",
    "| `id`      | `string` | 'UCCezIgC97PvUuR4_gbFUs5g', # Corey Schafer\n",
    "| `id`      | `string` | 'UCfzlCWGWYyIQ0aLC5w48gBQ', # Sentdex\n",
    "| `id`      | `string` | 'UCNU_lfiiWBdtULKOw6X0Dig', # Krish Naik\n",
    "| `id`      | `string` | 'UCzL_0nIe8B4-7ShhVPfJkgw', # DatascienceDoJo\n",
    "| `id`      | `string` | 'UCLLw7jmFsvfIVaUFsLs8mlQ', # Luke Barousse \n",
    "| `id`      | `string` | 'UCiT9RITQ9PW6BhXK0y2jaeg', # Ken Jee\n",
    "| `id`      | `string` | 'UC7cs8q-gJRlGwj4A8OmCmXg', # Alex the analyst\n",
    "| `id`      | `string` | 'UC2UXDak6o7rBm23k3Vv5dww', # Tina Huang\n",
    "\n",
    "\n",
    "#### add(more to follow)\n",
    "\n",
    "Takes two numbers and returns the sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Badges\n",
    "\n",
    "\n",
    "[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](https://choosealicense.com/licenses/mit/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link to top of page <a class='anchor' id='Top'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
